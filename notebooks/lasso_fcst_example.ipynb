{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b1df71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                # library for mathematical operations with arrays\n",
    "import pandas as pd               # library for data frames; includes useful functions for date arithmetic\n",
    "import xarray as xr               # library for arrays, especially tailored to weather data\n",
    "import matplotlib.pyplot as plt   # library for basic plotting\n",
    "import matplotlib.colors as mcolors \n",
    "import datetime                   # library for date manipulation\n",
    "\n",
    "from functools import reduce\n",
    "from scipy.stats import norm, pearsonr\n",
    "\n",
    "from sklearn.linear_model import MultiTaskLassoCV\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from warnings import simplefilter\n",
    "\n",
    "import os\n",
    "from os import path\n",
    "import sys\n",
    "sys.path.append(\"../src/confer_wp3/\")\n",
    "from plotting import plot_fields                                    # function for visualizing spatial data in a map\n",
    "from confer_wp3.dataloading import load_raw_data, save_anomalies, save_eofs_pcs\n",
    "from confer_wp3.lasso_forecast import calculate_anomalies, quantile_mapping, compute_eofs_pcs, get_all_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e44c795",
   "metadata": {},
   "source": [
    "The following two variables specify the paths where the forecast and observation data are stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d880ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_dir = '/nr/samba/PostClimDataNoBackup/CONFER/EASP/raw_predictors/'\n",
    "chirps_dir = '/nr/samba/PostClimDataNoBackup/CONFER/EASP/precip/chirps/'\n",
    "indices_dir = '/home/michael/nr/samba/PostClimDataNoBackup/CONFER/EASP/fls/predictors/'\n",
    "anomaly_dir = '/nr/samba/PostClimDataNoBackup/CONFER/EASP/precip/chirps/seasonal/halfdeg_res/'\n",
    "eof_dir = '/home/michael/nr/samba/PostClimDataNoBackup/CONFER/EASP/eofs/chirps/halfdeg_res/'\n",
    "fcst_dir = '/home/michael/nr/samba/PostClimDataNoBackup/CONFER/EASP/fls_pred/chirps/seasonal/halfdeg_res/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09afc40",
   "metadata": {},
   "source": [
    "Now, we set a number of parameters defining our forecast domain, training period, forecast year, etc.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e90cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_clm_start = 1993     # first year of the climatological reference period\n",
    "year_clm_end = 2020       # last year of the climatological reference period\n",
    "\n",
    "year_train_start = 1981   # first year of the training period\n",
    "year_train_end = 2020     # last year of the training period\n",
    "\n",
    "year_fcst = 2020          # year in which forecasts should be generated\n",
    "month_init = 8            # month in which the forecast should be generated (based on data of the preceding month)\n",
    "season = 'OND'\n",
    "\n",
    "lon_bnds = [20, 53]       # longitude range of the domain of interest\n",
    "lat_bnds = [-15, 23]      # latitude range of the domain of interest\n",
    "\n",
    "period_clm = [year_clm_start, year_clm_end]\n",
    "period_train = [year_train_start, year_train_end]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d06cd9",
   "metadata": {},
   "source": [
    "If not already available, load CHIRPS data, calculate seasonal precipitation anomalies, EOFs, and factor loading, and save out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45ebbcb",
   "metadata": {},
   "source": [
    "### CHIRPS Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2798304",
   "metadata": {},
   "source": [
    "##### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ac4018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary directory for storing results\n",
    "val_dir = \"/nr/samba/user/ahellevik/CONFER-WP3/validation_data/\"\n",
    "\n",
    "# Load data\n",
    "year, lat, lon, prec_data = load_raw_data(chirps_dir, \"chirps\", [*range(year_train_start, year_train_end+1)], season, lat_bnds, lon_bnds) # 1993, 2021\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa53666",
   "metadata": {},
   "source": [
    "##### Get anomalies and normalized anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2366b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get anomalies\n",
    "if not path.exists(f'{val_dir}chirps_anomalies.nc'):\n",
    "    # Calculate anomalies\n",
    "    anomalies = calculate_anomalies(prec_data, year, period_clm)\n",
    "    # Save anomalies\n",
    "    save_anomalies(anomalies, year, lat, lon, val_dir, normalized=False)\n",
    "else:\n",
    "    anomalies = xr.open_dataarray(f'{val_dir}chirps_anomalies.nc', engine='netcdf4')\n",
    "    anomalies = anomalies.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ac5666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get normalized anomalies\n",
    "if not path.exists(f'{val_dir}chirps_anomalies_normal.nc'):\n",
    "    # Apply the transformation to the anomalies data\n",
    "    anomalies_normal = quantile_mapping(anomalies, year, period_clm)\n",
    "    # Save normalized anomalies\n",
    "    save_anomalies(anomalies_normal, year, lat, lon, val_dir, normalized=True)\n",
    "else:\n",
    "    anomalies_normal = xr.open_dataarray(f'{val_dir}chirps_anomalies_normal.nc', engine='netcdf4')\n",
    "    anomalies_normal = anomalies_normal.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf0acc0",
   "metadata": {},
   "source": [
    "##### Get EOFs and factor loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898a9940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get EOFs\n",
    "if not path.exists(f'{val_dir}chirps_eofs.nc'):\n",
    "    # Calculate EOFs\n",
    "    n_eofs = 7  # Number of EOFs to compute\n",
    "    eofs, pcs, var_fracs = compute_eofs_pcs(anomalies_normal, n_eofs)\n",
    "    # Reshape EOFs to 3D (n_eofs, lat, lon)\n",
    "    eofs_reshaped = eofs.reshape((n_eofs, len(lat), len(lon)))\n",
    "    # Save EOFs, PCs and variance fractions\n",
    "    save_eofs_pcs(eofs_reshaped, pcs, var_fracs, year, lat, lon, val_dir)\n",
    "else:\n",
    "    eofs_reshaped = xr.open_dataarray(f'{val_dir}chirps_eofs.nc', engine='netcdf4').values\n",
    "    pcs = xr.open_dataarray(f'{val_dir}chirps_pcs.nc', engine='netcdf4').values\n",
    "    var_fracs = xr.open_dataarray(f'{val_dir}chirps_var_fracs.nc', engine='netcdf4').values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a655ea40",
   "metadata": {},
   "source": [
    "### ERA5 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7190f70",
   "metadata": {},
   "source": [
    "If not already available, load ERA5 data, calculate indices, and save out. Otherwise, load ERA5 data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec91ca0",
   "metadata": {},
   "source": [
    "##### Get ERA5 indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f581383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get era5 indices\n",
    "# Define the file path\n",
    "era5_indices_path = f'{val_dir}era5_indices.nc'\n",
    "# Shared between all indices\n",
    "months = list(range(1, 13))\n",
    "\n",
    "if not path.exists(era5_indices_path):\n",
    "    # Load needed data\n",
    "    load_years = [i for i in range(min(year_train_start, year_clm_start), max(year_train_end+1, year_clm_end+1))]\n",
    "    # Load sst data\n",
    "    sst_data = load_raw_data(era5_dir, \"sst\", load_years, season)\n",
    "    # Load uwind200 data\n",
    "    uwind200_data = load_raw_data(era5_dir, \"uwind200\", load_years, season)\n",
    "    # Load uwind850 data\n",
    "    uwind850_data = load_raw_data(era5_dir, \"uwind850\", load_years, season)\n",
    "\n",
    "    # Calculate indices\n",
    "    era5_indices = get_all_indices(sst_data, uwind200_data, uwind850_data, period_clm, period_train, months)\n",
    "    # Save indices\n",
    "    # Convert DataFrame to xarray Dataset for saving\n",
    "    ds = era5_indices.set_index(['year', 'month']).to_xarray()\n",
    "\n",
    "    # Save the Dataset to a NetCDF file\n",
    "    print(\"Saving indices...\")\n",
    "    ds.to_netcdf(era5_indices_path)\n",
    "    print(f\"Data saved to {era5_indices_path}\")\n",
    "else:\n",
    "    # Load the NetCDF file into an xarray Dataset\n",
    "    ds_loaded = xr.open_dataset(era5_indices_path, engine='netcdf4')\n",
    "    print(f\"Data loaded from {era5_indices_path}\")\n",
    "\n",
    "    # Convert the xarray Dataset back to a DataFrame\n",
    "    era5_indices = ds_loaded.to_dataframe().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f7d454",
   "metadata": {},
   "source": [
    "### ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680bddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precip data:\n",
    "# eofs_norm_anomalies_reshaped, numpy array of shape (eof = n_eofs, lat = 67, lon = 59) \n",
    "# pcs_ano_normal, numpy array of shape (year = train_end - train_start, n_eofs)\n",
    "# var_frac_ano_normal, numpy array of shape (n_eofs, )\n",
    "# Features/predictors: (dataframe with shape (year, month, standardized_anomaly))\n",
    "# time_series_{feature}_df for feature in ['n34','dmi','wvg','wsp','wpg','wp','wnp','n34_diff1','dmi_diff1','ueq850','ueq200','sji850','sji200']\n",
    "\n",
    "feature_names = ['n34','dmi','wvg','wsp','wpg','wp','wnp','n34_diff1','dmi_diff1','ueq850','ueq200','sji850','sji200']\n",
    "feature_dfs = {feature: eval(f\"time_series_{feature}_df\") for feature in feature_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910075a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplefilter(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc68b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation setup\n",
    "years_cv = list(range(year_train_start, year_train_end+1))\n",
    "years_verif = list(range(year_clm_start, year_clm_end+1))\n",
    "df_year = pd.DataFrame((np.array(years_cv) - 2000) / 10, index=years_cv, columns=['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bee072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of EOFs\n",
    "ntg = n_eofs\n",
    "\n",
    "# Calculate explained variance\n",
    "frac_expl_var = var_fracs / np.sum(var_fracs)\n",
    "wgt_values = np.sqrt(frac_expl_var)\n",
    "wgt = pd.DataFrame(np.tile(wgt_values, (len(years_verif), 1)), index=years_verif, columns=[f'eof{i+1}' for i in range(ntg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70753345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation folds\n",
    "\n",
    "k = 5\n",
    "nyr = len(years_cv)\n",
    "\n",
    "cv_folds = []\n",
    "for i in range(k):\n",
    "    idx_test = set(range(i*2, nyr, k*2)).union(set(range(1+i*2, nyr, k*2)))\n",
    "    idx_train = set(range(nyr)) - idx_test\n",
    "    cv_folds.append((list(idx_train), list(idx_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a148f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames for storing results\n",
    "df_fl_pred_mean = pd.DataFrame(index=years_verif, columns=[f'fl{i}' for i in range(1, ntg+1)])\n",
    "df_fl_pred_cov = pd.DataFrame(index=years_verif, columns=[f'cov-{i}{j}' for i in range(1, ntg+1) for j in range(1, ntg+1)])\n",
    "df_hyperparameters = pd.DataFrame(index=years_verif, columns=['alpha', 'l1_ratio'])\n",
    "df_selected_features = pd.DataFrame(index=years_verif, columns=['year'] + feature_names, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f696ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit models and make predictions\n",
    "previous_month = month_init - 1 if month_init > 1 else 12\n",
    "\n",
    "for iyr in years_verif:\n",
    "    df_target = pd.DataFrame(pcs[:, :ntg], index=years_cv).reindex(years_cv)\n",
    "    \n",
    "    df_combined_features = pd.DataFrame(index = years_cv)\n",
    "\n",
    "    # Select the data for the month before month_init and combine features\n",
    "    for feature in feature_names:\n",
    "        df_feature = feature_dfs[feature] \n",
    "        df_feature_selected = df_feature[df_feature['month'] == previous_month].set_index('year')['standardized_anomaly']\n",
    "        df_combined_features[feature] = df_feature_selected\n",
    "\n",
    "     # Add the standardized year as a predictor\n",
    "    # df_combined_features['year'] = (df_combined_features.index - 2000) / 10\n",
    "\n",
    "    # Ensure there are no missing values\n",
    "    df_combined_features.fillna(0, inplace=True)\n",
    "\n",
    "    y = df_target.to_numpy()\n",
    "    X = df_combined_features.to_numpy()\n",
    "\n",
    "    # Feature pre-selection\n",
    "    feature_idx = [True] + [False] * len(df_combined_features.columns)\n",
    "    for ift in range(len(feature_idx) - 1):\n",
    "        pval = [pearsonr(y[:, ipc], X[:, ift])[1] for ipc in range(ntg)]\n",
    "        feature_idx[1 + ift] = np.any(np.array(pval) < 0.1 * wgt.iloc[0, :])\n",
    "    \n",
    "    df_combined_features = df_combined_features.iloc[:, feature_idx[1:]]\n",
    "    df_year = pd.DataFrame((df_combined_features.index - 2000) / 10, index=df_combined_features.index, columns=['year'])\n",
    "    selected_columns = ['year'] + df_combined_features.columns.to_list()\n",
    "\n",
    "    # Ensure df_selected_features has the correct columns\n",
    "    if not set(selected_columns).issubset(df_selected_features.columns):\n",
    "        for col in selected_columns:\n",
    "            if col not in df_selected_features.columns:\n",
    "                df_selected_features[col] = 0\n",
    "\n",
    "    #print(f'{iyr}: {sum(feature_idx)} features selected')\n",
    "    if sum(feature_idx) == 1:\n",
    "        df_combined_features = df_year\n",
    "    else:\n",
    "        df_combined_features = pd.concat([df_year, df_combined_features], axis=1)\n",
    "\n",
    "    X = df_combined_features.to_numpy()\n",
    "    # Lasso regression\n",
    "    clf = MultiTaskLassoCV(cv=cv_folds, fit_intercept=False, max_iter=5000)\n",
    "    clf.fit(X, y)\n",
    "    df_hyperparameters.loc[iyr, 'alpha'] = clf.alpha_\n",
    "    df_hyperparameters.loc[iyr, 'l1_ratio'] = 1.0\n",
    "    ind_active = np.all(clf.coef_ != 0, axis=0)\n",
    "    #print(ind_active)\n",
    "    n_a = sum(ind_active)\n",
    "    #print(n_a)\n",
    "    if n_a > 0:\n",
    "        active_features = np.where(np.insert(ind_active, 0, True), 1, 0)  # Insert True for 'year' column\n",
    "        # Adjust the length of active_features to match selected_columns\n",
    "        if len(active_features) != len(selected_columns):\n",
    "            if len(active_features) < len(selected_columns):\n",
    "                active_features = np.append(active_features, [0] * (len(selected_columns) - len(active_features)))\n",
    "            else:\n",
    "                active_features = active_features[:len(selected_columns)]\n",
    "        \n",
    "        df_selected_features.loc[iyr, selected_columns] = active_features\n",
    "        dgf = 1 + n_a\n",
    "    else:\n",
    "        dgf = 1\n",
    "    df_coefficients = pd.DataFrame(clf.coef_, index=[f'eof{i}' for i in range(1,ntg+1)], columns=df_combined_features.columns)\n",
    "    #print(df_coefficients)\n",
    "    # Compute prediction covariance\n",
    "    errors = y - clf.predict(X)\n",
    "    df_fl_pred_cov.loc[iyr, :] = np.broadcast_to((np.dot(errors.T, errors) / (nyr - dgf)).flatten(), (1, ntg ** 2))\n",
    "\n",
    "#print(df_fl_pred_cov)\n",
    "# Save results\n",
    "# df_coefficients.to_csv(f'path_to_coefficients.csv')\n",
    "# df_fl_pred_cov.to_csv(f'path_to_fl_pred_cov.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65a1638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: CHIRPS vs. GPCC (short) vs. GPCC (long)\n",
    "\n",
    "month = 8\n",
    "season = 'OND'\n",
    "\n",
    "month_str = {1:\"January\", 2:\"February\", 3:\"March\", 4:\"April\", 5:\"May\", 6:\"June\", 7:\"July\", 8:\"August\", 9:\"September\", 10:\"October\", 11:\"November\", 12:\"December\"}[month]\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(14,4), width_ratios=[9, 8, 8])\n",
    "for i, target_prod, cv_period in zip([*range(3)],['chirps'],['1981-2020']):\n",
    "    df_coefficients = df_coefficients\n",
    "    nft = len(df_coefficients.columns)\n",
    "    ntg = len(df_coefficients.index)\n",
    "    img = ax[i].imshow(df_coefficients, vmin={'MAM':-80., 'JJAS':-160., 'OND':-230.}[season], vmax={'MAM':80., 'JJAS':160., 'OND':230.}[season], cmap='bwr', extent=[0,nft,0,ntg])\n",
    "    ax[i].set_xticks(np.arange(.5,nft))\n",
    "    ax[i].set_xticklabels(df_coefficients.columns.to_list(), rotation=90, fontsize=10)\n",
    "    ax[i].set_yticks(np.arange(ntg-.5,0,-1))\n",
    "    ax[i].set_yticklabels(df_coefficients.index.to_list(), fontsize=10)\n",
    "    ax[i].set_title(f'{target_prod.upper()}, training period: {cv_period}')\n",
    "\n",
    "fig.subplots_adjust(right=0.85)\n",
    "cbar_ax = fig.add_axes([0.9, 0.15, 0.03, 0.6])\n",
    "fig.colorbar(img, cax=cbar_ax)\n",
    "#fig.suptitle(f'LASSO regression coefficients for {season} forecast issued in {month_str}', fontsize=14)\n",
    "\n",
    "#plt.savefig(f'{data_dir}plots_ml_diagnostics/coef_training_period_comparison_im{month}_{season}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b53ddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tercile_probability_forecasts(season, year_fcst, month_init, period_clm, scaling, eofs, eigenvalues, coefficients, fl_eof_cov, ts_indices):\n",
    "\n",
    "    # Ensure columns and indices alignment\n",
    "    if not all(coefficients.columns == ts_indices.index):\n",
    "        raise ValueError(\"Columns of coefficients and indices of ts_indices are not aligned.\")\n",
    "    \n",
    "    # Calculate residual variance\n",
    "    var_eps = scaling**2 - np.sum(eigenvalues[:, None, None] * eofs**2, axis=0)\n",
    "\n",
    "    # Ensure ts_indices has year standardized\n",
    "    ts_indices['year'] = (year_fcst - 2000) / 10\n",
    "\n",
    "    # Calculate predictive mean of factor loadings\n",
    "    fl_eof_mean = coefficients.dot(ts_indices).to_numpy()\n",
    "\n",
    "    # Calculate mean and variance of the probabilistic forecast in normal space\n",
    "    mean_ml = np.sum(fl_eof_mean[:, None, None] * eofs, axis=0)\n",
    "    var_ml = np.sum(np.sum(fl_eof_cov[:, :, None, None] * eofs[None, :, :, :], axis=1) * eofs, axis=0) + var_eps\n",
    "\n",
    "    mean_ml = np.array(mean_ml, dtype=np.float64)\n",
    "    var_ml = np.array(var_ml, dtype=np.float64)\n",
    "\n",
    "    mean_ml_stdz = mean_ml / scaling\n",
    "    stdv_ml_stdz = np.sqrt(var_ml) / scaling\n",
    "\n",
    "    # Calculate tercile forecasts\n",
    "    prob_bn = norm.cdf((norm.ppf(0.333) - mean_ml_stdz) / stdv_ml_stdz)\n",
    "    prob_an = 1.0 - norm.cdf((norm.ppf(0.667) - mean_ml_stdz) / stdv_ml_stdz)\n",
    "\n",
    "    return prob_bn, prob_an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55f9ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simple(fields, titles, cmap, unit, year):\n",
    "    n_fields = len(fields)\n",
    "    fig, axes = plt.subplots(1, n_fields, figsize=(15, 5), subplot_kw={'projection': None})\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        im = ax.imshow(fields[i], extent=[lon.min(), lon.max(), lat.min(), lat.max()],\n",
    "                       origin='lower', cmap=cmap[i], vmin=0.333, vmax=1)\n",
    "        ax.set_title(titles[i])\n",
    "        ax.set_xlabel('Longitude')\n",
    "        ax.set_ylabel('Latitude')\n",
    "        cbar = fig.colorbar(im, ax=ax, orientation='vertical')\n",
    "        cbar.set_label(unit)\n",
    "\n",
    "    fig.suptitle(f'Predicted tercile probabilities for {season} precipitation amounts, {year}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd36294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percentiles(chirps_data, ref_period_indices):\n",
    "    # Extract the reference period indice\n",
    "    chirps_ref = chirps_data[ref_period_indices, :, :]\n",
    "    \n",
    "    # Calculate the 33rd and 67th percentiles for each grid point\n",
    "    percentile_33 = np.nanpercentile(chirps_ref, 33, axis=0)\n",
    "    percentile_67 = np.nanpercentile(chirps_ref, 67, axis=0)\n",
    "    \n",
    "    return percentile_33, percentile_67\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fea601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_precipitation(chirps_data, percentiles_33, percentiles_67, year):\n",
    "    # Select the specific year\n",
    "    actual_precip = chirps_data[year - year_train_start, :, :]\n",
    "    \n",
    "    # Categorize the precipitation\n",
    "    below_normal = actual_precip < percentiles_33\n",
    "    above_normal = actual_precip > percentiles_67\n",
    "    normal = (actual_precip >= percentiles_33) & (actual_precip <= percentiles_67)\n",
    "    \n",
    "    # Create a categorical array: 0 for below normal, 1 for normal, 2 for above normal\n",
    "    categories = xr.where(below_normal, 0, xr.where(above_normal, 2, 1))\n",
    "    \n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a2104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_predictions(prob_bn, prob_an, actual_categories):\n",
    "    # Determine the predicted category based on highest probability\n",
    "    pred_categories = xr.where(prob_bn > 0.4, 0, xr.where(prob_an > 0.4, 2, 1))\n",
    "    \n",
    "    # Compare predictions with actual categories\n",
    "    correct_predictions = pred_categories == actual_categories\n",
    "    \n",
    "    return correct_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520b420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_verification(verification, prec_data, lon, lat):\n",
    "    # Reintroduce NaNs based on the original prec_data\n",
    "    masked_verification = np.ma.masked_where(np.isnan(prec_data[year_fcst - year_train_start, :, :]), verification)\n",
    "    \n",
    "    # Create a custom colormap\n",
    "    cmap = mcolors.ListedColormap(['red', 'green', 'gray'])\n",
    "    bounds = [0, 0.5, 1.5, 2]\n",
    "    norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 5), subplot_kw={'projection': None})\n",
    "    \n",
    "    im = ax.imshow(masked_verification, extent=[lon.min(), lon.max(), lat.min(), lat.max()],\n",
    "                   origin='lower', cmap=cmap, norm=norm)\n",
    "    \n",
    "    ax.set_title(f'Verification of {season} {year_fcst} Precipitation Forecast')\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    \n",
    "    # Create a color bar with the correct labels\n",
    "    cbar = fig.colorbar(im, ax=ax, orientation='vertical', ticks=[0.25, 1, 1.75])\n",
    "    cbar.ax.set_yticklabels(['Incorrect', 'Correct', 'Masked'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7900c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "# Select the reference period using a boolean mask\n",
    "ref_period_mask = (year >= period_clm[0]) & (year <= period_clm[1])\n",
    "ref_period_indices = np.where(ref_period_mask)[0]\n",
    "# Season, year_fcst, month_init, period_train, period_clm, ntg predefined\n",
    "# Model coeffiecients in df_coefficients\n",
    "# previous_month = month_init - 1 if month_init > 1 else 12\n",
    "year_fcst = 2020\n",
    "for year_fcst in range(year_clm_start, year_clm_end+1):\n",
    "    # Reshape the covariance matrix for each year\n",
    "    def reshape_covariance_matrix(cov_df, ntg):\n",
    "        reshaped_cov = np.zeros((len(cov_df), ntg, ntg))\n",
    "        for i, year in enumerate(cov_df.index):\n",
    "            reshaped_cov[i] = cov_df.loc[year].values.reshape(ntg, ntg)\n",
    "        return reshaped_cov\n",
    "\n",
    "    df_fl_pred_cov_reshaped = reshape_covariance_matrix(df_fl_pred_cov, ntg)\n",
    "\n",
    "    # Use the reshaped covariance matrix with the right year\n",
    "    cov_matrix_for_year = df_fl_pred_cov_reshaped[years_verif.index(year_fcst)]\n",
    "\n",
    "    scaling = np.nanstd(anomalies_normal[ref_period_indices, :, :], axis = 0, ddof = 1)\n",
    "\n",
    "    # Prepare ts_indices\n",
    "    ts_indices = pd.Series(index=df_coefficients.columns)\n",
    "    for feature in feature_names:\n",
    "        df_feature = feature_dfs[feature]\n",
    "        df_feature_selected = df_feature[df_feature['month'] == previous_month].set_index('year')['standardized_anomaly']\n",
    "        ts_indices[feature] = df_feature_selected.loc[year_fcst]\n",
    "\n",
    "    eigenvalues = var_fracs  # The variance fraction for each EOF\n",
    "    # Ensure alignment before using the function\n",
    "    coefficients_columns = df_coefficients.columns.to_list()\n",
    "    ts_indices = ts_indices.reindex(coefficients_columns)\n",
    "\n",
    "    # Debugging statements\n",
    "    # print(\"Coefficient Columns:\", df_coefficients.columns)\n",
    "    # print(\"TS Indices Index:\", ts_indices.index)\n",
    "    # print(\"TS Indices:\", ts_indices)\n",
    "    # print(\"EOFs Shape:\", eofs_norm_anomalies_reshaped.shape)\n",
    "    # print(\"Eigenvalues Shape:\", eigenvalues.shape)\n",
    "    # print(\"Coefficients Shape:\", df_coefficients.shape)\n",
    "    # print(\"FL EOF Covariance Shape:\", df_fl_pred_cov_reshaped.shape)\n",
    "\n",
    "    prob_bn, prob_an = calculate_tercile_probability_forecasts(season, year_fcst, month_init, period_clm, scaling, eofs_reshaped, eigenvalues, df_coefficients, cov_matrix_for_year, ts_indices)\n",
    "\n",
    "    # Plot the probabilities\n",
    "    plot_simple(fields=[prob_bn, prob_an],\n",
    "                titles=['Below Normal', 'Above Normal'],\n",
    "                cmap=['Oranges', 'Greens'],\n",
    "                unit='Probability',\n",
    "                year=year_fcst)\n",
    "\n",
    "    # Define the reference period\n",
    "    reference_period = period_clm\n",
    "\n",
    "    # Calculate percentiles\n",
    "    percentiles_33, percentiles_67 = calculate_percentiles(prec_data, ref_period_indices)\n",
    "\n",
    "    actual_categories = categorize_precipitation(prec_data, percentiles_33, percentiles_67, year_fcst)\n",
    "    verification = verify_predictions(prob_bn, prob_an, actual_categories)\n",
    "\n",
    "    # Plot the verification\n",
    "    plot_verification(verification, prec_data, lon, lat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba52bb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from memory_profiler import profile\n",
    "\n",
    "def get_nearest_grid_index(lon_exmpl, lat_exmpl, lon_grid, lat_grid):\n",
    "    ix = np.argmin(abs(lon_grid-lon_exmpl))\n",
    "    iy = np.argmin(abs(lat_grid-lat_exmpl))\n",
    "    return ix, iy\n",
    "\n",
    "def get_xticks(x_extent, inc = 1):\n",
    "    x_inc = np.arange(-180,180,inc)\n",
    "    return(x_inc[np.where(np.logical_and(x_inc >= x_extent[0], x_inc <= x_extent[1]))])\n",
    "\n",
    "def get_yticks(y_extent, inc = 1):\n",
    "    y_inc = np.arange(-90,90,inc)\n",
    "    return(y_inc[np.where(np.logical_and(y_inc >= y_extent[0], y_inc <= y_extent[1]))])\n",
    "\n",
    "@profile\n",
    "def plot_fields(fields_list, lon, lat, lon_bounds, lat_bounds, main_title, subtitle_list, unit, vmin=None, vmax=None, cmap='BuPu', water_bodies=False, ticks=True, tick_labels=None):\n",
    "\n",
    "    n_img = len(fields_list)\n",
    "    img_extent = lon_bounds + lat_bounds\n",
    "\n",
    "    if not type(unit) is list:\n",
    "        unit = [unit for i in range(n_img)]\n",
    "\n",
    "    if not type(cmap) is list:\n",
    "        cmap = [cmap for i in range(n_img)]\n",
    "\n",
    "    if vmin == None:\n",
    "        vmin = [np.nanmin(field) for field in fields_list]\n",
    "    if vmax == None:\n",
    "        vmax = [np.nanmax(field) for field in fields_list]\n",
    "\n",
    "    if not type(vmin) is list:\n",
    "        vmin = [vmin for i in range(n_img)]\n",
    "    if not type(vmax) is list:\n",
    "        vmax = [vmax for i in range(n_img)]       \n",
    "\n",
    "    if ticks == True:\n",
    "        ticks = [ticks for i in range(n_img)]\n",
    "    elif not type(ticks) is list:\n",
    "        print(\"Error! Argument 'ticks' must be a list or a list of lists.\")\n",
    "    elif all([isinstance(tt, float) or isinstance(tt, int) for tt in ticks]):\n",
    "        ticks = [ticks for i in range(n_img)]\n",
    "\n",
    "    if tick_labels == None:\n",
    "        tick_labels = [tick_labels for i in range(n_img)]\n",
    "    elif not type(tick_labels) is list:\n",
    "        print(\"Error! Argument 'ticks_labels' must be a list or a list of lists.\")\n",
    "    elif all([isinstance(tt, float) or isinstance(tt, int) or isinstance(tt, str) for tt in tick_labels]):\n",
    "        tick_labels = [tick_labels for i in range(n_img)]\n",
    "\n",
    "    r = abs(lon[1]-lon[0])\n",
    "    lons_mat, lats_mat = np.meshgrid(lon, lat)\n",
    "    lons_matplot = np.hstack((lons_mat - r/2, lons_mat[:,[-1]] + r/2))\n",
    "    lons_matplot = np.vstack((lons_matplot, lons_matplot[[-1],:]))\n",
    "    lats_matplot = np.hstack((lats_mat, lats_mat[:,[-1]]))\n",
    "    lats_matplot = np.vstack((lats_matplot - r/2, lats_matplot[[-1],:] + r/2))     # assumes latitudes in ascending order\n",
    "\n",
    "    dlon = (lon_bounds[1]-lon_bounds[0]) // 8\n",
    "    dlat = (lat_bounds[1]-lat_bounds[0]) // 8\n",
    "\n",
    "    fig_height = 7.\n",
    "    fig_width = (n_img*1.15)*(fig_height/1.1)*np.diff(lon_bounds)[0]/np.diff(lat_bounds)[0]\n",
    "\n",
    "    fig = plt.figure(figsize=(fig_width,fig_height))\n",
    "    for i_img in range(n_img):\n",
    "        ax = fig.add_subplot(100+n_img*10+i_img+1, projection=ccrs.PlateCarree())\n",
    "        cmesh = ax.pcolormesh(lons_matplot, lats_matplot, fields_list[i_img], cmap=cmap[i_img], vmin=vmin[i_img], vmax=vmax[i_img])\n",
    "        ax.set_extent(img_extent, crs=ccrs.PlateCarree())\n",
    "        ax.set_yticks(get_yticks(img_extent[2:4],dlat), crs=ccrs.PlateCarree())\n",
    "        ax.yaxis.set_major_formatter(LatitudeFormatter()) \n",
    "        ax.set_xticks(get_xticks(img_extent[0:2],dlon), crs=ccrs.PlateCarree())\n",
    "        ax.xaxis.set_major_formatter(LongitudeFormatter(zero_direction_label=True))\n",
    "        ax.add_feature(cfeature.COASTLINE, linewidth=2)\n",
    "        ax.add_feature(cfeature.BORDERS, linewidth=2, linestyle='-', alpha=.9)\n",
    "        if water_bodies:\n",
    "            ax.add_feature(cfeature.LAKES, alpha=0.95)\n",
    "            ax.add_feature(cfeature.RIVERS)\n",
    "\n",
    "        plt.title(subtitle_list[i_img], fontsize=14)\n",
    "        divider = make_axes_locatable(ax)\n",
    "        ax_cb = divider.new_horizontal(size=\"5%\", pad=0.1, axes_class=plt.Axes)\n",
    "        fig.add_axes(ax_cb)\n",
    "        cbar = plt.colorbar(cmesh, cax=ax_cb)\n",
    "        cbar.set_label(unit[i_img])\n",
    "        if not ticks[i_img] == True:\n",
    "            cbar.set_ticks(ticks[i_img])\n",
    "            cbar.set_ticklabels(tick_labels[i_img])\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    plt.tight_layout(rect=[0,0,1,0.95])\n",
    "    fig.suptitle(main_title, fontsize=16)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cbcc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_fields(fields_list = [prob_bn, prob_an],\n",
    "#           lon = lon,\n",
    "#           lat = lat,\n",
    "#           lon_bounds = lon_bnds,\n",
    "#           lat_bounds = lat_bnds,\n",
    "#           main_title = f'Predicted tercile probabilities for {season} precipitation amounts',\n",
    "#           subtitle_list = ['below normal','above normal'],\n",
    "#           vmin = 0.333,\n",
    "#           vmax = 1,\n",
    "#           cmap = ['Oranges','Greens'],\n",
    "#           unit = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccd1eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#data_dir = '/home/michael/nr/samba/PostClimDataNoBackup/CONFER/EASP/'\n",
    "\n",
    "# month = month_init                # month in which the forecast is issued; predictors are taken from previous month\n",
    "# season_name = season\n",
    "# ref_period = f'{year_clm_start}-{year_clm_end}'\n",
    "\n",
    "\n",
    "# print(f'\\n Generating {season} forecasts at halfdeg resolution')\n",
    "\n",
    "# # -- Calculate explained variance by the respective EOFs-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# refper_start = int(ref_period[:4])\n",
    "# refper_end = int(ref_period[-4:])\n",
    "\n",
    "# nc = xr.open_dataset(f'{data_dir}eofs/chirps/halfdeg_res/refper_{ref_period}/prec_loyo_seasonal_{season}.nc')\n",
    "# nc_subset = nc.sel(loy=slice(years_verif[0],years_verif[-1]))\n",
    "# eigenvalues = (nc_subset.d.values**2) / (refper_end-refper_start)\n",
    "# nc.close()\n",
    "\n",
    "# for iyr in years_verif:\n",
    "# #    print(iyr)\n",
    "#     df_target = data_prec.loc[(slice(None),iyr,range(1,ntg+1)),:].unstack().droplevel(level=1).droplevel(level=0, axis=1).fillna(0.).reindex(years_cv)\n",
    "#     data_indices_yr = []\n",
    "#     for i in range(len(data_indices)):\n",
    "#         data_indices_yr.append(data_indices[i].loc[(slice(None),month-1,iyr),:].droplevel(level=(1,2)))\n",
    "#     df_features = pd.concat(data_indices_yr, axis=1).fillna(0.).reindex(years_cv)\n",
    "#     y = df_target.to_numpy()\n",
    "#     X = df_features.to_numpy()\n",
    "#    # Feature pre-selection\n",
    "#     feature_idx = [True]+[False]*len(df_features.columns)\n",
    "#     for ift in range(len(feature_idx)-1):\n",
    "#         pval = [pearsonr(y[:,ipc], X[:,ift])[1] for ipc in range(ntg)]\n",
    "#         feature_idx[1+ift] = np.any(np.array(pval)<0.1*wgt.loc[iyr])\n",
    "#     df_features = df_features.iloc[:,feature_idx[1:]]\n",
    "#     df_selected_features.loc[iyr,['year']+df_features.columns.to_list()] = 0\n",
    "#     print(f'{iyr}: {sum(feature_idx)} features selected')\n",
    "#    # Interactions with b/n/a dummy variables of features\n",
    "#     if sum(feature_idx) == 1:\n",
    "#         df_features = df_year\n",
    "#     else:\n",
    "#         df_features = pd.concat([df_year, df_features], axis=1)\n",
    "\n",
    "#     X = df_features.to_numpy()\n",
    "\n",
    "#     # Lasso regression of pre-selected features\n",
    "#     clf = MultiTaskLassoCV(cv=cv_folds, fit_intercept=False, max_iter=5000)\n",
    "#     clf.fit(X, y)\n",
    "#     df_hyperparameters.loc[iyr,'alpha'] = clf.alpha_\n",
    "#     df_hyperparameters.loc[iyr,'l1_ratio'] = 1.\n",
    "#     ind_active = np.all(clf.coef_!=0, axis=0)\n",
    "#     n_a = sum(ind_active)\n",
    "#     if n_a > 0:\n",
    "#         df_selected_features.loc[iyr,df_features.columns.to_list()] = np.where(ind_active, 1, 0)\n",
    "#         dgf = 1 + n_a\n",
    "#     else:\n",
    "#         dgf = 1\n",
    "#    # In 'full' mode, we mainly care about the estimated coefficients\n",
    "#     df_coefficients = pd.DataFrame(clf.coef_, index=[f'eof{i}' for i in range(1,ntg+1)], columns=df_features.columns)\n",
    "#     errors = y - clf.predict(X)\n",
    "#     df_fl_pred_cov.loc[:,:] = np.broadcast_to((np.dot(errors.T,errors)/(nyr-dgf)).flatten(), (len(years_verif),ntg**2))\n",
    "#     break\n",
    "\n",
    "\n",
    "# # if not path.exists(f'{data_dir}fls_pred/chirps/seasonal/halfdeg_res'):\n",
    "# #     os.mkdir(f'{data_dir}fls_pred/chirps/seasonal/halfdeg_res')\n",
    "\n",
    "# # if not path.exists(f'{data_dir}fls_pred/chirps/seasonal/halfdeg_res/refper_{ref_period}_cvper_{years_cv[0]}-{years_cv[-1]}'):\n",
    "#     os.mkdir(f'{data_dir}fls_pred/chirps/seasonal/halfdeg_res/refper_{ref_period}_cvper_{years_cv[0]}-{years_cv[-1]}')\n",
    "\n",
    "# df_coefficients.to_csv(f'{data_dir}fls_pred/chirps/seasonal/halfdeg_res/refper_{ref_period}_cvper_{years_cv[0]}-{years_cv[-1]}/coefficients_{predictor}_lasso_full_im{month}_{season}.csv')\n",
    "# df_fl_pred_cov.to_csv(f'{data_dir}fls_pred/chirps/seasonal/halfdeg_res/refper_{ref_period}_cvper_{years_cv[0]}-{years_cv[-1]}/fls_cov_{predictor}_lasso_full_im{month}_{season}.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47ca337",
   "metadata": {},
   "source": [
    "Run LASSO model to predict precipitation EOFs based on indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37aca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ec6f69",
   "metadata": {},
   "source": [
    "Visualize fitted coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950e58c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e7471e",
   "metadata": {},
   "source": [
    "Load indices for the forecast year and use the previously fitted model to make a forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9853a8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prob_fcst_below, prob_fcst_above = calculate_tercile_probability_forecasts(season, year_fcst, month_init, period_train, period_clm, indices_dir, anomaly_dir, eof_dir, fcst_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e326602",
   "metadata": {},
   "source": [
    "Depict as a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cf4aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_fields (fields_list = [prob_fcst_below, prob_fcst_above],\n",
    "#           lon = lon,\n",
    "#           lat = lat,\n",
    "#           lon_bounds = lon_bnds,\n",
    "#           lat_bounds = lat_bnds,\n",
    "#           main_title = f'Predicted tercile probabilities for {season} precipitation amounts',\n",
    "#           subtitle_list = ['below normal','above normal'],\n",
    "#           vmin = 0.333,\n",
    "#           vmax = 1,\n",
    "#           cmap = ['Oranges','Greens'],\n",
    "#           unit = '')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (confer-wp3-env)",
   "language": "python",
   "name": "confer-wp3-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
