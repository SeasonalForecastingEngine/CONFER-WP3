{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b1df71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                # library for mathematical operations with arrays\n",
    "import pandas as pd               # library for data frames; includes useful functions for date arithmetic\n",
    "import xarray as xr               # library for arrays, especially tailored to weather data\n",
    "import matplotlib.pyplot as plt   # library for basic plotting\n",
    "import matplotlib.colors as mcolors \n",
    "import datetime                   # library for date manipulation\n",
    "\n",
    "from functools import reduce\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from sklearn.linear_model import MultiTaskLassoCV\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from warnings import simplefilter\n",
    "\n",
    "from os import path\n",
    "import sys\n",
    "sys.path.append(\"../src/confer_wp3/\")\n",
    "from confer_wp3.dataloading import load_raw_data, save_anomalies, save_eofs_pcs\n",
    "from confer_wp3.validation import validate_anomalies1, validate_anomalies2, validate_anomalies3, validate_anomalies4, validate_eofs, validate_pcs\n",
    "from confer_wp3.lasso_forecast import calculate_anomalies, compute_eofs_pcs, quantile_mapping, standardize_index, standardize_index_diff1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e44c795",
   "metadata": {},
   "source": [
    "The following two variables specify the paths where the forecast and observation data are stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d880ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_dir = '/nr/samba/PostClimDataNoBackup/CONFER/EASP/raw_predictors/'\n",
    "chirps_dir = '/nr/samba/PostClimDataNoBackup/CONFER/EASP/precip/chirps/'\n",
    "indices_dir = '/home/michael/nr/samba/PostClimDataNoBackup/CONFER/EASP/fls/predictors/'\n",
    "anomaly_dir = '/nr/samba/PostClimDataNoBackup/CONFER/EASP/precip/chirps/seasonal/halfdeg_res/'\n",
    "eof_dir = '/home/michael/nr/samba/PostClimDataNoBackup/CONFER/EASP/eofs/chirps/halfdeg_res/'\n",
    "fcst_dir = '/home/michael/nr/samba/PostClimDataNoBackup/CONFER/EASP/fls_pred/chirps/seasonal/halfdeg_res/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09afc40",
   "metadata": {},
   "source": [
    "Now, we set a number of parameters defining our forecast domain, training period, forecast year, etc.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e90cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_clm_start = 1993     # first year of the climatological reference period\n",
    "year_clm_end = 2020       # last year of the climatological reference period\n",
    "\n",
    "year_train_start = 1981   # first year of the training period\n",
    "year_train_end = 2020     # last year of the training period\n",
    "\n",
    "year_fcst = 2020          # year in which forecasts should be generated\n",
    "month_init = 8            # month in which the forecast should be generated (based on data of the preceding month)\n",
    "season = 'OND'\n",
    "\n",
    "lon_bnds = [20, 53]       # longitude range of the domain of interest\n",
    "lat_bnds = [-15, 23]      # latitude range of the domain of interest\n",
    "\n",
    "period_clm = [year_clm_start, year_clm_end]\n",
    "period_train = [year_train_start, year_train_end]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d06cd9",
   "metadata": {},
   "source": [
    "If not already available, load CHIRPS data, calculate seasonal precipitation anomalies, EOFs, and factor loading, and save out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45ebbcb",
   "metadata": {},
   "source": [
    "### CHIRPS Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2798304",
   "metadata": {},
   "source": [
    "##### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ac4018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary directory for storing results\n",
    "val_dir = \"/nr/samba/user/ahellevik/CONFER-WP3/validation_data/\"\n",
    "\n",
    "# Load data\n",
    "year, lat, lon, prec_data = load_raw_data(chirps_dir, \"chirps\", [*range(year_train_start, year_train_end+1)], season, lat_bnds, lon_bnds) # 1993, 2021\n",
    "prec_data.shape\n",
    "print(\"Loaded data shape:\", prec_data.shape)\n",
    "print(\"NaN values in loaded data:\", np.isnan(prec_data).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa53666",
   "metadata": {},
   "source": [
    "##### Get/calculate anomalies and normalized anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2366b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get anomalies\n",
    "if not path.exists(f'{val_dir}chirps_anomalies.nc'):\n",
    "    # Calculate anomalies\n",
    "    anomalies = calculate_anomalies(prec_data, year, period_clm)\n",
    "    # Save anomalies\n",
    "    save_anomalies(anomalies, year, lat, lon, val_dir, normalized=False)\n",
    "else:\n",
    "    anomalies = xr.open_dataarray(f'{val_dir}chirps_anomalies.nc', engine='netcdf4')\n",
    "    anomalies = anomalies.values\n",
    "print(prec_data.shape)\n",
    "print(anomalies.shape)\n",
    "# Verify NaNs are handled correctly\n",
    "print(\"NaN values in prec_data:\", np.isnan(prec_data).sum())\n",
    "print(\"NaN values in anomalies:\", np.isnan(anomalies).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc57a641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get normalized anomalies\n",
    "if not path.exists(f'{val_dir}chirps_anomalies_normal.nc'):\n",
    "    # Apply the transformation to the anomalies data\n",
    "    anomalies_normal = quantile_mapping(anomalies, year, period_clm)\n",
    "    # Save normalized anomalies\n",
    "    save_anomalies(anomalies_normal, year, lat, lon, val_dir, normalized=True)\n",
    "else:\n",
    "    anomalies_normal = xr.open_dataarray(f'{val_dir}chirps_anomalies_normal.nc', engine='netcdf4')\n",
    "    anomalies_normal = anomalies_normal.values\n",
    "print(anomalies_normal.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a64a03",
   "metadata": {},
   "source": [
    "##### Plots to ensure calculating anomalies went well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9927e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_anomalies1(prec_data, anomalies, lat, lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acbad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_anomalies2(anomalies, anomalies_normal, lat, lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2701cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_anomalies3(anomalies, anomalies_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63438ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_anomalies4(anomalies, anomalies_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf0acc0",
   "metadata": {},
   "source": [
    "##### Get/calculate EOFs and factor loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e1e944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get EOFs\n",
    "if not path.exists(f'{val_dir}chirps_eofs.nc'):\n",
    "    # Calculate EOFs\n",
    "    n_eofs = 7  # Number of EOFs to compute\n",
    "    eofs, pcs, var_fracs = compute_eofs_pcs(anomalies_normal, n_eofs)\n",
    "    # Reshape EOFs to 3D (n_eofs, lat, lon)\n",
    "    eofs_reshaped = eofs.reshape((n_eofs, len(lat), len(lon)))\n",
    "    # Save EOFs, PCs and variance fractions\n",
    "    save_eofs_pcs(eofs_reshaped, pcs, var_fracs, year, lat, lon, val_dir, n_eofs)\n",
    "else:\n",
    "    eofs_reshaped = xr.open_dataarray(f'{val_dir}chirps_eofs.nc', engine='netcdf4').values\n",
    "    pcs = xr.open_dataarray(f'{val_dir}chirps_pcs.nc', engine='netcdf4').values\n",
    "    var_fracs = xr.open_dataarray(f'{val_dir}chirps_var_fracs.nc', engine='netcdf4').values\n",
    "    \n",
    "print(\"Normalized Anomalies EOFs Shape:\", eofs_reshaped.shape)\n",
    "print(\"Normalized Anomalies PCs Shape:\", pcs.shape)\n",
    "print(\"Normalized Anomalies Variance Fraction:\", var_fracs, \"sum: \", var_fracs.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e956ac",
   "metadata": {},
   "source": [
    "##### Plots to ensure calculating EOFs and factor loadings went well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03246e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot EOFs for normalized anomalies\n",
    "validate_eofs(eofs_reshaped, f\"Normalized Anomalies - {season}\", n_eofs=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038be677",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_pcs(anomalies_normal, eofs_reshaped, pcs, lat, lon, year, period_train, season, n_eofs = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a655ea40",
   "metadata": {},
   "source": [
    "### ERA5 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7190f70",
   "metadata": {},
   "source": [
    "If not already available, load ERA5 data, calculate indices, and save out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678430a2",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f1ac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_years = [*range(min(year_train_start, year_clm_start), max(year_train_end+1, year_clm_end+1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb9236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sst data\n",
    "sst_data = load_raw_data(era5_dir, \"sst\", load_years, season)\n",
    "sst_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ef6d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load uwind200 data\n",
    "uwind200_data = load_raw_data(era5_dir, \"uwind200\", load_years, season)\n",
    "uwind200_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcfe812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load uwind850 data\n",
    "uwind850_data = load_raw_data(era5_dir, \"uwind850\", load_years, season)\n",
    "uwind850_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec91ca0",
   "metadata": {},
   "source": [
    "### Calculate indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630fa5d1",
   "metadata": {},
   "source": [
    "#### Functions for index calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c1d0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_region_indices(region):\n",
    "    \"\"\"\n",
    "    Retrieve the bounding box coordinates for a specified region.\n",
    "\n",
    "    Parameters:\n",
    "    - region (str): The name of the region for which to retrieve the bounding box coordinates.\n",
    "      Valid region names include:\n",
    "        - 'n34': Nino 3.4 region\n",
    "        - 'n3': Nino 3 region\n",
    "        - 'n4_1': Nino 4 (part 1) region\n",
    "        - 'n4_2': Nino 4 (part 2) region\n",
    "        - 'wpg': Western Pacific region\n",
    "        - 'dmi_1': Dipole Mode Index (West)\n",
    "        - 'dmi_2': Dipole Mode Index (East)\n",
    "        - 'sji850': South Indian Ocean Jet at 850 hPa\n",
    "        - 'sji200': South Indian Ocean Jet at 200 hPa\n",
    "        - 'ueq850': Upper Equatorial region at 850 hPa\n",
    "        - 'ueq200': Upper Equatorial region at 200 hPa\n",
    "        - 'wp': Western Pacific region\n",
    "        - 'wnp_1': Western North Pacific (part 1)\n",
    "        - 'wnp_2': Western North Pacific (part 2)\n",
    "        - 'wsp_1': Western South Pacific (part 1)\n",
    "        - 'wsp_2': Western South Pacific (part 2)\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing the bounding box coordinates for the specified region, with keys:\n",
    "        - 'lat_min': Minimum latitude\n",
    "        - 'lat_max': Maximum latitude\n",
    "        - 'lon_min': Minimum longitude\n",
    "        - 'lon_max': Maximum longitude\n",
    "    \"\"\"\n",
    "    # Define the bounding boxes for the indices\n",
    "    indices_definitions = {\n",
    "        'n34': {'lat_min': -5, 'lat_max': 5, 'lon_min': -170, 'lon_max': -120},\n",
    "        'n3': {'lat_min': -5, 'lat_max': 5, 'lon_min': -150, 'lon_max': -90},\n",
    "        'n4_1': {'lat_min': -5, 'lat_max': 5, 'lon_min': -180, 'lon_max': -150},\n",
    "        'n4_2': {'lat_min': -5, 'lat_max': 5, 'lon_min': 160, 'lon_max': 180},\n",
    "        'wpg': {'lat_min': 0, 'lat_max': 20, 'lon_min': 130, 'lon_max': 150},\n",
    "        'dmi_1': {'lat_min': -10, 'lat_max': 10, 'lon_min': 50, 'lon_max': 70}, # West\n",
    "        'dmi_2': {'lat_min': -10, 'lat_max': 0, 'lon_min': 90, 'lon_max': 110}, # East\n",
    "        'sji850': {'lat_min': 0, 'lat_max': 15, 'lon_min': 35, 'lon_max': 50},\n",
    "        'sji200': {'lat_min': 0, 'lat_max': 15, 'lon_min': 35, 'lon_max': 50},\n",
    "        'ueq850': {'lat_min': -4, 'lat_max': 4, 'lon_min': 60, 'lon_max': 90},\n",
    "        'ueq200': {'lat_min': -4, 'lat_max': 4, 'lon_min': 60, 'lon_max': 90},\n",
    "        'wp' : {'lat_min': -15, 'lat_max': 20, 'lon_min': 120, 'lon_max': 160},\n",
    "        'wnp_1' : {'lat_min': 20, 'lat_max': 35, 'lon_min': 160, 'lon_max': 180},\n",
    "        'wnp_2' : {'lat_min': 20, 'lat_max': 35, 'lon_min': -180, 'lon_max': -150},\n",
    "        'wsp_1' : {'lat_min': -30, 'lat_max': -15, 'lon_min': 155, 'lon_max': 180}, \n",
    "        'wsp_2' : {'lat_min': -30, 'lat_max': -15, 'lon_min': -180, 'lon_max': -150},\n",
    "    }\n",
    "    # Get index for region\n",
    "    return indices_definitions[region]\n",
    "\n",
    "\n",
    "def standardize_index(data, index_name, period_clm, year_fcst, month_init, before = False):\n",
    "    # Check if trying to calculate for first month and year\n",
    "    first_year = data['year'].min().item()\n",
    "    if year_fcst == first_year and month_init == 1:\n",
    "        raise ValueError(\"Cannot calculate the index for the first month in the first year of the dataset.\")\n",
    "    \n",
    "    # Calculate the index for the entire dataset\n",
    "    if index_name in [\"n4\", \"dmi\", \"wnp\", \"wsp\"]: \n",
    "        region_1 = get_region_indices(f\"{index_name}_1\")\n",
    "        region_2 = get_region_indices(f\"{index_name}_2\")\n",
    "        subset_1 = data.sel(lat=slice(region_1['lat_min'], region_1['lat_max']), lon=slice(region_1['lon_min'], region_1['lon_max']))\n",
    "        subset_2 = data.sel(lat=slice(region_2['lat_min'], region_2['lat_max']), lon=slice(region_2['lon_min'], region_2['lon_max']))\n",
    "        \n",
    "        if index_name == \"dmi\":\n",
    "            index_dmi_1 = subset_1.mean(dim=['lat', 'lon'])\n",
    "            index_dmi_2 = subset_2.mean(dim=['lat', 'lon'])\n",
    "            \n",
    "            if before:\n",
    "                # Select the reference period from the indices\n",
    "                ref_data_dmi_1 = index_dmi_1.sel(year=slice(period_clm[0], period_clm[1]))\n",
    "                ref_data_dmi_2 = index_dmi_2.sel(year=slice(period_clm[0], period_clm[1]))\n",
    "\n",
    "                # Calculate the climatology (mean) and standard deviation during the reference period for both indices\n",
    "                climatology_dmi_1 = ref_data_dmi_1.mean(dim='year')\n",
    "                climatology_std_dmi_1 = ref_data_dmi_1.std(dim='year', ddof=1)\n",
    "                climatology_dmi_2 = ref_data_dmi_2.mean(dim='year')\n",
    "                climatology_std_dmi_2 = ref_data_dmi_2.std(dim='year', ddof=1)\n",
    "\n",
    "                # Standardize the entire index data\n",
    "                standardized_index_dmi_1 = (index_dmi_1 - climatology_dmi_1) / climatology_std_dmi_1\n",
    "                standardized_index_dmi_2 = (index_dmi_2 - climatology_dmi_2) / climatology_std_dmi_2\n",
    "\n",
    "                # Calculate the difference between the standardized indices\n",
    "                index = standardized_index_dmi_1 - standardized_index_dmi_2\n",
    "            else:\n",
    "                # Calculate the difference between the indices\n",
    "                index = index_dmi_1 - index_dmi_2\n",
    "        else:\n",
    "            combined_subset = xr.concat([subset_1, subset_2], dim='lat')\n",
    "            index = combined_subset.mean(dim=['lat', 'lon'])\n",
    "\n",
    "    elif index_name == \"wpg\":\n",
    "        # Calculate the index for the entire dataset\n",
    "        region_wp = get_region_indices(index_name)\n",
    "        region_n4_1 = get_region_indices(\"n4_1\")\n",
    "        region_n4_2 = get_region_indices(\"n4_2\")\n",
    "        subset_wp = data.sel(lat=slice(region_wp['lat_min'], region_wp['lat_max']), lon=slice(region_wp['lon_min'], region_wp['lon_max']))\n",
    "        subset_n4_1 = data.sel(lat=slice(region_n4_1['lat_min'], region_n4_1['lat_max']), lon=slice(region_n4_1['lon_min'], region_n4_1['lon_max']))\n",
    "        subset_n4_2 = data.sel(lat=slice(region_n4_2['lat_min'], region_n4_2['lat_max']), lon=slice(region_n4_2['lon_min'], region_n4_2['lon_max']))\n",
    "        combined_subset_n4 = xr.concat([subset_n4_1, subset_n4_2], dim='lat')\n",
    "        \n",
    "        index_wp = subset_wp.mean(dim=['lat', 'lon'])\n",
    "        index_n4 = combined_subset_n4.mean(dim=['lat', 'lon'])\n",
    "\n",
    "        if before:\n",
    "            # Select the reference period from the indices\n",
    "            ref_data_wp = index_wp.sel(year=slice(period_clm[0], period_clm[1]))\n",
    "            ref_data_n4 = index_n4.sel(year=slice(period_clm[0], period_clm[1]))\n",
    "\n",
    "            # Calculate the climatology (mean) and standard deviation during the reference period for both indices\n",
    "            climatology_wp = ref_data_wp.mean(dim='year')\n",
    "            climatology_std_wp = ref_data_wp.std(dim='year', ddof=1)\n",
    "            climatology_n4 = ref_data_n4.mean(dim='year')\n",
    "            climatology_std_n4 = ref_data_n4.std(dim='year', ddof=1)\n",
    "\n",
    "            # Standardize the entire index data\n",
    "            standardized_index_wp = (index_wp - climatology_wp) / climatology_std_wp\n",
    "            standardized_index_n4 = (index_n4 - climatology_n4) / climatology_std_n4\n",
    "\n",
    "            # Calculate the difference between the standardized indices\n",
    "            index = standardized_index_n4 - standardized_index_wp\n",
    "        else:\n",
    "            # Calculate the difference between the indices\n",
    "            index = index_n4 - index_wp\n",
    "\n",
    "    else:\n",
    "        region = get_region_indices(index_name)\n",
    "        subset = data.sel(lat=slice(region['lat_min'], region['lat_max']), lon=slice(region['lon_min'], region['lon_max']))\n",
    "        index = subset.mean(dim=['lat', 'lon'])\n",
    "\n",
    "    if before:\n",
    "        if month_init == 1:\n",
    "            current_datapoint = index.sel(year=year_fcst-1, month=12)\n",
    "        else:\n",
    "            current_datapoint = index.sel(year=year_fcst, month=month_init-1)\n",
    "\n",
    "        return current_datapoint\n",
    "    \n",
    "    else:\n",
    "        # Select the reference period from the index\n",
    "        ref_data = index.sel(year=slice(period_clm[0], period_clm[1]))\n",
    "\n",
    "        # Calculate the climatology (mean) and standard deviation during the reference period\n",
    "        climatology = ref_data.mean(dim=['year'])\n",
    "        climatology_std = ref_data.std(dim=['year'], ddof = 1)\n",
    "\n",
    "        # Standardize the current data point based on data from previous month\n",
    "        if month_init == 1:\n",
    "            current_datapoint = index.sel(year = year_fcst-1, month = 12)\n",
    "            climatology = climatology.sel(month = 12)\n",
    "            climatology_std = climatology_std.sel(month = 12)\n",
    "        else:\n",
    "            current_datapoint = index.sel(year = year_fcst, month = month_init-1)\n",
    "            climatology = climatology.sel(month = month_init-1)\n",
    "            climatology_std = climatology_std.sel(month = month_init-1)\n",
    "        \n",
    "        anomalies = current_datapoint - climatology\n",
    "        standardized_anomalies = anomalies / climatology_std\n",
    "        \n",
    "        return standardized_anomalies\n",
    "\n",
    "\n",
    "def standardize_index_diff1(data, index_name, period_clm, year_fcst, month_init, before = False):\n",
    "    # Check if trying to calculate for first month and year\n",
    "    first_year = data['year'].min().item()\n",
    "    if year_fcst == first_year and month_init == 1:\n",
    "        raise ValueError(\"Cannot calculate the index for the first month in the first year of the dataset.\")\n",
    "    \n",
    "    # Calculate the index for the entire dataset\n",
    "    if index_name == \"n34\":\n",
    "        region = get_region_indices(index_name)\n",
    "        subset = data.sel(lat=slice(region['lat_min'], region['lat_max']), lon=slice(region['lon_min'], region['lon_max']))\n",
    "        index = subset.mean(dim=['lat', 'lon'])\n",
    "    elif index_name == \"dmi\":\n",
    "        region_dmi_1 = get_region_indices(\"dmi_1\")\n",
    "        region_dmi_2 = get_region_indices(\"dmi_2\")\n",
    "        subset_dmi_1 = data.sel(lat=slice(region_dmi_1['lat_min'], region_dmi_1['lat_max']), lon=slice(region_dmi_1['lon_min'], region_dmi_1['lon_max']))\n",
    "        subset_dmi_2 = data.sel(lat=slice(region_dmi_2['lat_min'], region_dmi_2['lat_max']), lon=slice(region_dmi_2['lon_min'], region_dmi_2['lon_max']))\n",
    "        index_dmi_1 = subset_dmi_1.mean(dim=['lat', 'lon'])\n",
    "        index_dmi_2 = subset_dmi_2.mean(dim=['lat', 'lon'])\n",
    "\n",
    "        if before:\n",
    "                # Select the reference period from the indices\n",
    "                ref_data_dmi_1 = index_dmi_1.sel(year=slice(period_clm[0], period_clm[1]))\n",
    "                ref_data_dmi_2 = index_dmi_2.sel(year=slice(period_clm[0], period_clm[1]))\n",
    "\n",
    "                # Calculate the climatology (mean) and standard deviation during the reference period for both indices\n",
    "                climatology_dmi_1 = ref_data_dmi_1.mean(dim='year')\n",
    "                climatology_std_dmi_1 = ref_data_dmi_1.std(dim='year', ddof=1)\n",
    "                climatology_dmi_2 = ref_data_dmi_2.mean(dim='year')\n",
    "                climatology_std_dmi_2 = ref_data_dmi_2.std(dim='year', ddof=1)\n",
    "\n",
    "                # Standardize the entire index data\n",
    "                standardized_index_dmi_1 = (index_dmi_1 - climatology_dmi_1) / climatology_std_dmi_1\n",
    "                standardized_index_dmi_2 = (index_dmi_2 - climatology_dmi_2) / climatology_std_dmi_2\n",
    "\n",
    "                # Calculate the difference between the standardized indices\n",
    "                index = standardized_index_dmi_1 - standardized_index_dmi_2\n",
    "\n",
    "        else:\n",
    "            # Calculate the difference between the indices\n",
    "            index = index_dmi_1 - index_dmi_2\n",
    "    else:\n",
    "        print(f\"Diff1 not implemented for index {index_name}\")\n",
    "        raise TypeError(f\"Diff1 not implemented for index {index_name}\")\n",
    "\n",
    "    # Select the reference period from the index\n",
    "    ref_data = index.sel(year=slice(period_clm[0], period_clm[1]))\n",
    "\n",
    "    if before:\n",
    "        # Calculate the climatology (mean) and standard deviation during the reference period\n",
    "        climatology = ref_data.mean(dim='year')\n",
    "        climatology_std = ref_data.std(dim='year', ddof=1)\n",
    "\n",
    "        # Standardize the entire index data\n",
    "        standardized_index = (index - climatology) / climatology_std\n",
    "\n",
    "        # Calculate the differences for all 12 months, including January using December from the previous year\n",
    "        diff_list = []\n",
    "        for year in range(period_clm[0], period_clm[1] + 1):\n",
    "            for month in range(1, 13):\n",
    "                if month == 1:\n",
    "                    if year > period_clm[0]:  # Ensure we have the previous year's December data\n",
    "                        current_value = standardized_index.sel(year=year, month=1)\n",
    "                        previous_value = standardized_index.sel(year=year-1, month=12)\n",
    "                        diff = (current_value - previous_value).assign_coords(year=year, month=month)\n",
    "                        diff_list.append(diff)\n",
    "                else:\n",
    "                    current_value = standardized_index.sel(year=year, month=month)\n",
    "                    previous_value = standardized_index.sel(year=year, month=month-1)\n",
    "                    diff = (current_value - previous_value).assign_coords(year=year, month=month)\n",
    "                    diff_list.append(diff)\n",
    "\n",
    "    else:\n",
    "        # Calculate the differences for all 12 months, including January using December from the previous year\n",
    "        diff_list = []\n",
    "        \n",
    "        for year in range(period_clm[0], period_clm[1] + 1):\n",
    "            for month in range(1, 13):\n",
    "                if month == 1:\n",
    "                    if year > period_clm[0]:  # Ensure we have the previous year's December data\n",
    "                        current_value = ref_data.sel(year=year, month=1)\n",
    "                        previous_value = ref_data.sel(year=year-1, month=12)\n",
    "                        diff = (current_value - previous_value).assign_coords(year=year, month=month)\n",
    "                        diff_list.append(diff)\n",
    "                else:\n",
    "                    current_value = ref_data.sel(year=year, month=month)\n",
    "                    previous_value = ref_data.sel(year=year, month=month-1)\n",
    "                    diff = (current_value - previous_value).assign_coords(year=year, month=month)\n",
    "                    diff_list.append(diff)\n",
    "    \n",
    "    diff_data = xr.concat(diff_list, dim='time')\n",
    "\n",
    "    climatology = diff_data.mean(dim='time')\n",
    "    climatology_std = diff_data.std(dim='time', ddof = 1)\n",
    "\n",
    "    if before:\n",
    "        # Standardize the current data point based on data from the previous month\n",
    "        if month_init == 1:\n",
    "            current_datapoint = standardized_index.sel(year = year_fcst, month = 1)\n",
    "            previous_datapoint = standardized_index.sel(year = year_fcst-1, month = 12)\n",
    "        else:\n",
    "            current_datapoint = standardized_index.sel(year = year_fcst, month = month_init)\n",
    "            previous_datapoint = standardized_index.sel(year = year_fcst, month = month_init-1)\n",
    "    else:\n",
    "        # Standardize the current data point based on data from the previous month\n",
    "        if month_init == 1:\n",
    "            current_datapoint = index.sel(year = year_fcst, month = 1)\n",
    "            previous_datapoint = index.sel(year = year_fcst-1, month = 12)\n",
    "        else:\n",
    "            current_datapoint = index.sel(year = year_fcst, month = month_init)\n",
    "            previous_datapoint = index.sel(year = year_fcst, month = month_init-1)\n",
    "\n",
    "    # Calculate the difference between the current and previous month\n",
    "    difference = current_datapoint - previous_datapoint\n",
    "\n",
    "    anomalies = difference - climatology\n",
    "    standardized_anomalies = anomalies / climatology_std\n",
    "    \n",
    "    return standardized_anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed506ac",
   "metadata": {},
   "source": [
    "#### Code for plotting time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91051957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_time_series_data(data, index_name, period_clm, period_train, months, diff1 = False):\n",
    "    time_series_data = []\n",
    "\n",
    "    for year in range(period_train[0], period_train[1] + 1):\n",
    "        # print(f\"Preparing data for year {year}\")\n",
    "        if (year == period_train[0]) & (months[0] == 1):\n",
    "            months_loop = months[1:]\n",
    "        else:\n",
    "            months_loop = months\n",
    "        for month in months_loop:\n",
    "            if diff1:\n",
    "                standardized_anomaly = standardize_index_diff1(data, index_name, period_clm, year, month, before = True)\n",
    "            else:\n",
    "                standardized_anomaly = standardize_index(data, index_name, period_clm, year, month)\n",
    "            if index_name in [\"ueq850\", \"ueq200\", \"sji850\", \"sji200\"]:\n",
    "                standardized_anomaly = standardized_anomaly.uwind.values\n",
    "            else:\n",
    "                standardized_anomaly = standardized_anomaly.sst.values\n",
    "            if month == 1:\n",
    "                year_prev = year - 1\n",
    "                month_prev = 12\n",
    "            else:\n",
    "                year_prev = year\n",
    "                month_prev = month - 1\n",
    "\n",
    "            time_series_data.append({\n",
    "                'year': year_prev,\n",
    "                'month': month_prev,\n",
    "                'standardized_anomaly': standardized_anomaly\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(time_series_data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ec882f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_reference_index(reference_df, period_train):\n",
    "    # Filter the reference dataframe for the specified time range\n",
    "    reference_df_filtered = reference_df[(reference_df['year'] >= period_train[0]) & (reference_df['year'] <= period_train[1])]\n",
    "    reference_df_filtered = reference_df_filtered[['year', 'month', 'fl']]\n",
    "    # Drop duplicate rows\n",
    "    reference_df_filtered = reference_df_filtered.drop_duplicates()\n",
    "    return reference_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a4f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series(df, index_name, comparison = False):\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month']].assign(day=1))\n",
    "    df = df.sort_values('date')\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    if comparison:\n",
    "        plt.plot(df['date'], df['standardized_anomaly'], label=f\"{index_name} Calculated\", color='b')\n",
    "        plt.plot(df['date'], df['fl'], label=f\"{index_name} Reference\", color='r', linestyle='--')\n",
    "    else:\n",
    "        plt.plot(df['date'], df['standardized_anomaly'], label=f\"{index_name} Index\", color='b')\n",
    "    plt.title(f\"Time Series of {index_name} Index\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(f\"{index_name} Index Value\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5f4cf4",
   "metadata": {},
   "source": [
    "#### Calculating and verifying indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b39fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared between all indices\n",
    "months = list(range(1, 13))\n",
    "start_year = year_train_start # Add fixing in standardize_index for the first month of the first year, which fails now\n",
    "end_year = year_train_end\n",
    "forecast_year = year_fcst\n",
    "filepath_indices = f'/nr/samba/PostClimDataNoBackup/CONFER/EASP/fls/predictors/refper_1993-2020/indices/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0535c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['n34','n3','n4','dmi','n34_diff1','dmi_diff1','wsp','wpg','wp','wnp','ueq850','ueq200','sji850','sji200']\n",
    "time_series_n34_df = prepare_time_series_data(sst_data, \"n34\", period_clm, period_train, months)\n",
    "feature_dfs = {}\n",
    "for feature in feature_names:\n",
    "    if feature.endswith(\"diff1\"):\n",
    "        time_series_df = prepare_time_series_data(sst_data, feature[:3], period_clm, period_train, months, diff1=True)\n",
    "    elif feature in ['ueq850','sji850']:\n",
    "        prepare_time_series_data(uwind850_data, feature, period_clm, period_train, months)\n",
    "    elif feature in ['ueq200','sji200']:\n",
    "        prepare_time_series_data(uwind200_data, feature, period_clm, period_train, months)\n",
    "    else:\n",
    "        time_series_df = prepare_time_series_data(sst_data, feature, period_clm, period_train, months)\n",
    "    feature_dfs[feature] = time_series_df\n",
    "# Add wvg as well\n",
    "# Merge DataFrames\n",
    "merge_for_wvg_df = reduce(lambda left, right: pd.merge(left, right, on=['year', 'month']), (\n",
    "    feature_dfs[\"n4\"].rename(columns={'standardized_anomaly': 'n4'}),\n",
    "    feature_dfs[\"wp\"].rename(columns={'standardized_anomaly': 'wp'}),\n",
    "    feature_dfs[\"wnp\"].rename(columns={'standardized_anomaly': 'wnp'}),\n",
    "    feature_dfs[\"wsp\"].rename(columns={'standardized_anomaly': 'wsp'}))\n",
    ")\n",
    "merge_for_wvg_df['standardized_anomaly'] = merge_for_wvg_df['n4'] - (merge_for_wvg_df['wp'] + merge_for_wvg_df['wnp'] + merge_for_wvg_df['wsp']) / 3\n",
    "feature_dfs[\"wvg\"] = merge_for_wvg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894de817",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a68945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each DataFrame to xarray DataArray\n",
    "dataarrays = {}\n",
    "for feature, df in feature_dfs.items():\n",
    "    df['year'] = df['year'].astype(int)\n",
    "    df['month'] = df['month'].astype(int)\n",
    "    df.set_index(['year', 'month'], inplace=True)\n",
    "    dataarrays[feature] = df.to_xarray()['standardized_anomaly']\n",
    "\n",
    "# Combine DataArrays into a Dataset\n",
    "ds = xr.Dataset(dataarrays)\n",
    "\n",
    "# Define the file path\n",
    "era5_indices_path = f'{val_dir}era5_indices.nc'\n",
    "\n",
    "# Save the Dataset to a NetCDF file\n",
    "ds.to_netcdf(era5_indices_path)\n",
    "print(f\"Data saved to {era5_indices_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2728f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the NetCDF file into an xarray Dataset\n",
    "# ds_loaded = xr.open_dataset(netcdf_file_path)\n",
    "# print(f\"Data loaded from {netcdf_file_path}\")\n",
    "\n",
    "# # Convert the xarray Dataset back to a DataFrame\n",
    "# df_loaded = ds_loaded.to_dataframe().reset_index()\n",
    "\n",
    "# # Print the first few rows of the DataFrame to verify\n",
    "# print(df_loaded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75def75c",
   "metadata": {},
   "source": [
    "##### Index N34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ab6e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_n34_df = prepare_time_series_data(sst_data, \"n34\", period_clm, period_train, months)\n",
    "\n",
    "# Reference values for index n34\n",
    "n34_index_reference = pd.read_csv(f\"{filepath_indices}n34_full.csv\")\n",
    "time_series_n34_reference_df = process_reference_index(n34_index_reference, period_train)\n",
    "\n",
    "# Merge and plot dataframes\n",
    "merged_df = pd.merge(time_series_n34_df, time_series_n34_reference_df, on=['year', 'month'], suffixes=('_calculated', '_reference'))\n",
    "plot_time_series(merged_df, \"n34\", comparison = True)\n",
    "\n",
    "# Print some values for comparison\n",
    "for month in range(1, 3):\n",
    "    print(f\"Standardized anomaly (calculated index) for forecast year: {forecast_year}, forecast month = {month} (based on data from month before):\")\n",
    "    print(standardize_index(sst_data, \"n34\", period_clm, year_fcst=forecast_year, month_init=month).sst.values)\n",
    "    print(f\"Reference value:\")\n",
    "    if month == 1:\n",
    "        print(n34_index_reference[(n34_index_reference[\"year\"] == forecast_year-1) & (n34_index_reference[\"month\"] == 12)].fl.values[0])\n",
    "    else:\n",
    "        print(n34_index_reference[(n34_index_reference[\"year\"] == forecast_year) & (n34_index_reference[\"month\"] == month-1)].fl.values[0])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5c3ae2",
   "metadata": {},
   "source": [
    "##### Index N3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3852dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_n3_df = prepare_time_series_data(sst_data, \"n3\", period_clm, period_train, months)\n",
    "\n",
    "# Reference values for index n3\n",
    "n3_index_reference = pd.read_csv(f\"{filepath_indices}n3_full.csv\")\n",
    "time_series_n3_reference_df = process_reference_index(n3_index_reference, period_train)\n",
    "\n",
    "# Merge and plot dataframes\n",
    "merged_df = pd.merge(time_series_n3_df, time_series_n3_reference_df, on=['year', 'month'], suffixes=('_calculated', '_reference'))\n",
    "plot_time_series(merged_df, \"n3\", comparison = True)\n",
    "\n",
    "# Print some values for comparison\n",
    "for month in range(1, 3):\n",
    "    print(f\"Standardized anomaly (calculated index) for forecast year: {forecast_year}, forecast month = {month} (based on data from month before):\")\n",
    "    print(standardize_index(sst_data, \"n3\", period_clm, year_fcst=forecast_year, month_init=month).sst.values)\n",
    "    print(f\"Reference value:\")\n",
    "    if month == 1:\n",
    "        print(n3_index_reference[(n3_index_reference[\"year\"] == forecast_year-1) & (n3_index_reference[\"month\"] == 12)].fl.values[0])\n",
    "    else:\n",
    "        print(n3_index_reference[(n3_index_reference[\"year\"] == forecast_year) & (n3_index_reference[\"month\"] == month-1)].fl.values[0])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fb7e74",
   "metadata": {},
   "source": [
    "##### Index N4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da32991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_n4_df = prepare_time_series_data(sst_data, \"n4\", period_clm, period_train, months)\n",
    "\n",
    "# Reference values for index n4\n",
    "n4_index_reference = pd.read_csv(f\"{filepath_indices}n4_full.csv\")\n",
    "time_series_n4_reference_df = process_reference_index(n4_index_reference, period_train)\n",
    "\n",
    "# Merge and plot dataframes\n",
    "merged_df = pd.merge(time_series_n4_df, time_series_n4_reference_df, on=['year', 'month'], suffixes=('_calculated', '_reference'))\n",
    "plot_time_series(merged_df, \"n4\", comparison = True)\n",
    "\n",
    "# Print some values for comparison\n",
    "for month in range(1, 3):\n",
    "    print(f\"Standardized anomaly (calculated index) for forecast year: {forecast_year}, forecast month = {month} (based on data from month before):\")\n",
    "    print(standardize_index(sst_data, \"n4\", period_clm, year_fcst=forecast_year, month_init=month).sst.values)\n",
    "    print(f\"Reference value:\")\n",
    "    if month == 1:\n",
    "        print(n4_index_reference[(n4_index_reference[\"year\"] == forecast_year-1) & (n4_index_reference[\"month\"] == 12)].fl.values[0])\n",
    "    else:\n",
    "        print(n4_index_reference[(n4_index_reference[\"year\"] == forecast_year) & (n4_index_reference[\"month\"] == month-1)].fl.values[0])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa70bd28",
   "metadata": {},
   "source": [
    "##### Index WPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19c0760",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_wpg_df = prepare_time_series_data(sst_data, \"wpg\", period_clm, period_train, months)\n",
    "\n",
    "# Reference values for index wpg\n",
    "wpg_index_reference = pd.read_csv(f\"{filepath_indices}wpg_full.csv\")\n",
    "time_series_wpg_reference_df = process_reference_index(wpg_index_reference, period_train)\n",
    "\n",
    "# Merge and plot dataframes\n",
    "merged_df = pd.merge(time_series_wpg_df, time_series_wpg_reference_df, on=['year', 'month'], suffixes=('_calculated', '_reference'))\n",
    "plot_time_series(merged_df, \"wpg\", comparison = True)\n",
    "\n",
    "# Print some values for comparison\n",
    "for month in range(1, 3):\n",
    "    print(f\"Standardized anomaly (calculated index) for forecast year: {forecast_year}, forecast month = {month} (based on data from month before):\")\n",
    "    print(standardize_index(sst_data, \"wpg\", period_clm, year_fcst=forecast_year, month_init=month, before = True).sst.values)\n",
    "    print(f\"Reference value:\")\n",
    "    if month == 1:\n",
    "        print(wpg_index_reference[(wpg_index_reference[\"year\"] == forecast_year-1) & (wpg_index_reference[\"month\"] == 12)].fl.values[0])\n",
    "    else:\n",
    "        print(wpg_index_reference[(wpg_index_reference[\"year\"] == forecast_year) & (wpg_index_reference[\"month\"] == month-1)].fl.values[0])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a270ecbf",
   "metadata": {},
   "source": [
    "##### Index DMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a829589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_dmi_df = prepare_time_series_data(sst_data, \"dmi\", period_clm, period_train, months)\n",
    "\n",
    "# Reference values for index dmi\n",
    "dmi_index_reference = pd.read_csv(f\"{filepath_indices}dmi_full.csv\")\n",
    "time_series_dmi_reference_df = process_reference_index(dmi_index_reference, period_train)\n",
    "\n",
    "# Merge and plot dataframes\n",
    "merged_df = pd.merge(time_series_dmi_df, time_series_dmi_reference_df, on=['year', 'month'], suffixes=('_calculated', '_reference'))\n",
    "plot_time_series(merged_df, \"dmi\", comparison = True)\n",
    "\n",
    "# Print some values for comparison\n",
    "for month in range(1, 3):\n",
    "    print(f\"Standardized anomaly (calculated index) for forecast year: {forecast_year}, forecast month = {month} (based on data from month before):\")\n",
    "    print(standardize_index(sst_data, \"dmi\", period_clm, year_fcst=forecast_year, month_init=month, before = True).sst.values)\n",
    "    print(f\"Reference value:\")\n",
    "    if month == 1:\n",
    "        print(dmi_index_reference[(dmi_index_reference[\"year\"] == forecast_year-1) & (dmi_index_reference[\"month\"] == 12)].fl.values[0])\n",
    "    else:\n",
    "        print(dmi_index_reference[(dmi_index_reference[\"year\"] == forecast_year) & (dmi_index_reference[\"month\"] == month-1)].fl.values[0])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587911ae",
   "metadata": {},
   "source": [
    "##### Index SJI850"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bda6cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_sji850_df = prepare_time_series_data(uwind850_data, \"sji850\", period_clm, period_train, months)\n",
    "\n",
    "# Reference values for index sji850\n",
    "sji850_index_reference = pd.read_csv(f\"{filepath_indices}sji850_full.csv\")\n",
    "time_series_sji850_reference_df = process_reference_index(sji850_index_reference, period_train)\n",
    "\n",
    "# Merge and plot dataframes\n",
    "merged_df = pd.merge(time_series_sji850_df, time_series_sji850_reference_df, on=['year', 'month'], suffixes=('_calculated', '_reference'))\n",
    "plot_time_series(merged_df, \"sji850\", comparison = True)\n",
    "\n",
    "# Print some values for comparison\n",
    "for month in range(1, 3):\n",
    "    print(f\"Standardized anomaly (calculated index) for forecast year: {forecast_year}, forecast month = {month} (based on data from month before):\")\n",
    "    print(standardize_index(uwind850_data, \"sji850\", period_clm, year_fcst=forecast_year, month_init=month).uwind.values)\n",
    "    print(f\"Reference value:\")\n",
    "    if month == 1:\n",
    "        print(sji850_index_reference[(sji850_index_reference[\"year\"] == forecast_year-1) & (sji850_index_reference[\"month\"] == 12)].fl.values[0])\n",
    "    else:\n",
    "        print(sji850_index_reference[(sji850_index_reference[\"year\"] == forecast_year) & (sji850_index_reference[\"month\"] == month-1)].fl.values[0])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3730f507",
   "metadata": {},
   "source": [
    "##### Index SJI200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40152eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_sji200_df = prepare_time_series_data(uwind200_data, \"sji200\", period_clm, period_train, months)\n",
    "\n",
    "# Reference values for index sji200\n",
    "sji200_index_reference = pd.read_csv(f\"{filepath_indices}sji200_full.csv\")\n",
    "time_series_sji200_reference_df = process_reference_index(sji200_index_reference, period_train)\n",
    "\n",
    "# Merge and plot dataframes\n",
    "merged_df = pd.merge(time_series_sji200_df, time_series_sji200_reference_df, on=['year', 'month'], suffixes=('_calculated', '_reference'))\n",
    "plot_time_series(merged_df, \"sji200\", comparison = True)\n",
    "\n",
    "# Print some values for comparison\n",
    "for month in range(1, 3):\n",
    "    print(f\"Standardized anomaly (calculated index) for forecast year: {forecast_year}, forecast month = {month} (based on data from month before):\")\n",
    "    print(standardize_index(uwind200_data, \"sji200\", period_clm, year_fcst=forecast_year, month_init=month).uwind.values)\n",
    "    print(f\"Reference value:\")\n",
    "    if month == 1:\n",
    "        print(sji200_index_reference[(sji200_index_reference[\"year\"] == forecast_year-1) & (sji200_index_reference[\"month\"] == 12)].fl.values[0])\n",
    "    else:\n",
    "        print(sji200_index_reference[(sji200_index_reference[\"year\"] == forecast_year) & (sji200_index_reference[\"month\"] == month-1)].fl.values[0])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e62e26",
   "metadata": {},
   "source": [
    "##### Index UEQ850"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a88ba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_ueq850_df = prepare_time_series_data(uwind850_data, \"ueq850\", period_clm, period_train, months)\n",
    "\n",
    "# Reference values for index ueq850\n",
    "ueq850_index_reference = pd.read_csv(f\"{filepath_indices}ueq850_full.csv\")\n",
    "time_series_ueq850_reference_df = process_reference_index(ueq850_index_reference, period_train)\n",
    "\n",
    "# Merge and plot dataframes\n",
    "merged_df = pd.merge(time_series_ueq850_df, time_series_ueq850_reference_df, on=['year', 'month'], suffixes=('_calculated', '_reference'))\n",
    "plot_time_series(merged_df, \"ueq850\", comparison = True)\n",
    "\n",
    "# Print some values for comparison\n",
    "for month in range(1, 3):\n",
    "    print(f\"Standardized anomaly (calculated index) for forecast year: {forecast_year}, forecast month = {month} (based on data from month before):\")\n",
    "    print(standardize_index(uwind850_data, \"ueq850\", period_clm, year_fcst=forecast_year, month_init=month).uwind.values)\n",
    "    print(f\"Reference value:\")\n",
    "    if month == 1:\n",
    "        print(ueq850_index_reference[(ueq850_index_reference[\"year\"] == forecast_year-1) & (ueq850_index_reference[\"month\"] == 12)].fl.values[0])\n",
    "    else:\n",
    "        print(ueq850_index_reference[(ueq850_index_reference[\"year\"] == forecast_year) & (ueq850_index_reference[\"month\"] == month-1)].fl.values[0])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd70d9c",
   "metadata": {},
   "source": [
    "##### Index UEQ200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d824dd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_ueq200_df = prepare_time_series_data(uwind200_data, \"ueq200\", period_clm, period_train, months)\n",
    "\n",
    "# Reference values for index ueq200\n",
    "ueq200_index_reference = pd.read_csv(f\"{filepath_indices}ueq200_full.csv\")\n",
    "time_series_ueq200_reference_df = process_reference_index(ueq200_index_reference, period_train)\n",
    "\n",
    "# Merge and plot dataframes\n",
    "merged_df = pd.merge(time_series_ueq200_df, time_series_ueq200_reference_df, on=['year', 'month'], suffixes=('_calculated', '_reference'))\n",
    "plot_time_series(merged_df, \"ueq200\", comparison = True)\n",
    "\n",
    "# Print some values for comparison\n",
    "for month in range(1, 3):\n",
    "    print(f\"Standardized anomaly (calculated index) for forecast year: {forecast_year}, forecast month = {month} (based on data from month before):\")\n",
    "    print(standardize_index(uwind200_data, \"ueq200\", period_clm, year_fcst=forecast_year, month_init=month).uwind.values)\n",
    "    print(f\"Reference value:\")\n",
    "    if month == 1:\n",
    "        print(ueq200_index_reference[(ueq200_index_reference[\"year\"] == forecast_year-1) & (ueq200_index_reference[\"month\"] == 12)].fl.values[0])\n",
    "    else:\n",
    "        print(ueq200_index_reference[(ueq200_index_reference[\"year\"] == forecast_year) & (ueq200_index_reference[\"month\"] == month-1)].fl.values[0])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552d57f0",
   "metadata": {},
   "source": [
    "##### Index WP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781c4551",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_wp_df = prepare_time_series_data(sst_data, \"wp\", period_clm, period_train, months)\n",
    "\n",
    "# Reference values for index wp\n",
    "wp_index_reference = pd.read_csv(f\"{filepath_indices}wp_full.csv\")\n",
    "time_series_wp_reference_df = process_reference_index(wp_index_reference, period_train)\n",
    "\n",
    "# Merge and plot dataframes\n",
    "merged_df = pd.merge(time_series_wp_df, time_series_wp_reference_df, on=['year', 'month'], suffixes=('_calculated', '_reference'))\n",
    "plot_time_series(merged_df, \"wp\", comparison = True)\n",
    "\n",
    "# Print some values for comparison\n",
    "for month in range(1, 3):\n",
    "    print(f\"Standardized anomaly (calculated index) for forecast year: {forecast_year}, forecast month = {month} (based on data from month before):\")\n",
    "    print(standardize_index(sst_data, \"wp\", period_clm, year_fcst=forecast_year, month_init=month).sst.values)\n",
    "    print(f\"Reference value:\")\n",
    "    if month == 1:\n",
    "        print(wp_index_reference[(wp_index_reference[\"year\"] == forecast_year-1) & (wp_index_reference[\"month\"] == 12)].fl.values[0])\n",
    "    else:\n",
    "        print(wp_index_reference[(wp_index_reference[\"year\"] == forecast_year) & (wp_index_reference[\"month\"] == month-1)].fl.values[0])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167a9c68",
   "metadata": {},
   "source": [
    "##### Index WNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f940614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_wnp_df = prepare_time_series_data(sst_data, \"wnp\", period_clm, period_train, months)\n",
    "\n",
    "# Reference values for index wnp\n",
    "wnp_index_reference = pd.read_csv(f\"{filepath_indices}wnp_full.csv\")\n",
    "time_series_wnp_reference_df = process_reference_index(wnp_index_reference, period_train)\n",
    "\n",
    "# Merge and plot dataframes\n",
    "merged_df = pd.merge(time_series_wnp_df, time_series_wnp_reference_df, on=['year', 'month'], suffixes=('_calculated', '_reference'))\n",
    "plot_time_series(merged_df, \"wnp\", comparison = True)\n",
    "\n",
    "# Print some values for comparison\n",
    "for month in range(1, 3):\n",
    "    print(f\"Standardized anomaly (calculated index) for forecast year: {forecast_year}, forecast month = {month} (based on data from month before):\")\n",
    "    print(standardize_index(sst_data, \"wnp\", period_clm, year_fcst=forecast_year, month_init=month).sst.values)\n",
    "    print(f\"Reference value:\")\n",
    "    if month == 1:\n",
    "        print(wnp_index_reference[(wnp_index_reference[\"year\"] == forecast_year-1) & (wnp_index_reference[\"month\"] == 12)].fl.values[0])\n",
    "    else:\n",
    "        print(wnp_index_reference[(wnp_index_reference[\"year\"] == forecast_year) & (wnp_index_reference[\"month\"] == month-1)].fl.values[0])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02d3161",
   "metadata": {},
   "source": [
    "##### Index WSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1681f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_wsp_df = prepare_time_series_data(sst_data, \"wsp\", period_clm, period_train, months)\n",
    "\n",
    "# Reference values for index wsp\n",
    "wsp_index_reference = pd.read_csv(f\"{filepath_indices}wsp_full.csv\")\n",
    "time_series_wsp_reference_df = process_reference_index(wsp_index_reference, period_train)\n",
    "\n",
    "# Merge and plot dataframes\n",
    "merged_df = pd.merge(time_series_wsp_df, time_series_wsp_reference_df, on=['year', 'month'], suffixes=('_calculated', '_reference'))\n",
    "plot_time_series(merged_df, \"wsp\", comparison = True)\n",
    "\n",
    "# Print some values for comparison\n",
    "for month in range(1, 3):\n",
    "    print(f\"Standardized anomaly (calculated index) for forecast year: {forecast_year}, forecast month = {month} (based on data from month before):\")\n",
    "    print(standardize_index(sst_data, \"wsp\", period_clm, year_fcst=forecast_year, month_init=month).sst.values)\n",
    "    print(f\"Reference value:\")\n",
    "    if month == 1:\n",
    "        print(wsp_index_reference[(wsp_index_reference[\"year\"] == forecast_year-1) & (wsp_index_reference[\"month\"] == 12)].fl.values[0])\n",
    "    else:\n",
    "        print(wsp_index_reference[(wsp_index_reference[\"year\"] == forecast_year) & (wsp_index_reference[\"month\"] == month-1)].fl.values[0])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3525d267",
   "metadata": {},
   "source": [
    "##### Index WVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365cd968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate index\n",
    "\n",
    "# Merge DataFrames\n",
    "merge_for_wvg_df = reduce(lambda left, right: pd.merge(left, right, on=['year', 'month']), (\n",
    "    time_series_n4_df.rename(columns={'standardized_anomaly': 'n4'}),\n",
    "    time_series_wp_df.rename(columns={'standardized_anomaly': 'wp'}),\n",
    "    time_series_wnp_df.rename(columns={'standardized_anomaly': 'wnp'}),\n",
    "    time_series_wsp_df.rename(columns={'standardized_anomaly': 'wsp'}))\n",
    ")\n",
    "merge_for_wvg_df['standardized_anomaly'] = merge_for_wvg_df['n4'] - (merge_for_wvg_df['wp'] + merge_for_wvg_df['wnp'] + merge_for_wvg_df['wsp']) / 3\n",
    "time_series_wvg_df = merge_for_wvg_df\n",
    "\n",
    "# Reference values for index wvg\n",
    "wvg_index_reference = pd.read_csv(f\"{filepath_indices}wvg_full.csv\")\n",
    "time_series_wvg_reference_df = process_reference_index(wvg_index_reference, period_train)\n",
    "\n",
    "# Plot dataframes\n",
    "merged_df = pd.merge(time_series_wvg_df, time_series_wvg_reference_df, on=['year', 'month'], suffixes=('_calculated', '_reference'))\n",
    "plot_time_series(merged_df, \"wvg\", comparison = True)\n",
    "\n",
    "# Print some values for comparison\n",
    "for month in range(1, 3):\n",
    "    print(f\"Standardized anomaly (calculated index) for forecast year: {forecast_year}, forecast month = {month} (based on data from month before):\")\n",
    "    n4 = standardize_index(sst_data, \"n4\", period_clm, year_fcst=forecast_year, month_init=month).sst.values\n",
    "    wp = standardize_index(sst_data, \"wp\", period_clm, year_fcst=forecast_year, month_init=month).sst.values\n",
    "    wnp = standardize_index(sst_data, \"wnp\", period_clm, year_fcst=forecast_year, month_init=month).sst.values\n",
    "    wsp = standardize_index(sst_data, \"wsp\", period_clm, year_fcst=forecast_year, month_init=month).sst.values\n",
    "    wvg = n4 - (wp + wnp + wsp) / 3\n",
    "    print(wvg)\n",
    "    print(f\"Reference value:\")\n",
    "    if month == 1:\n",
    "        print(wvg_index_reference[(wvg_index_reference[\"year\"] == forecast_year-1) & (wvg_index_reference[\"month\"] == 12)].fl.values[0])\n",
    "    else:\n",
    "        print(wvg_index_reference[(wvg_index_reference[\"year\"] == forecast_year) & (wvg_index_reference[\"month\"] == month-1)].fl.values[0])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba54e95",
   "metadata": {},
   "source": [
    "##### Index N34_DIFF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275bf1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_n34_diff1_df = prepare_time_series_data(sst_data, \"n34\", period_clm, period_train, months, diff1 = True)\n",
    "\n",
    "# Reference values for index n34_diff1\n",
    "n34_diff1_index_reference = pd.read_csv(f\"{filepath_indices}n34_diff1_full.csv\")\n",
    "time_series_n34_diff1_reference_df = process_reference_index(n34_diff1_index_reference, period_train)\n",
    "\n",
    "# Merge and plot dataframes\n",
    "merged_df = pd.merge(time_series_wpg_df, time_series_wpg_reference_df, on=['year', 'month'], suffixes=('_calculated', '_reference'))\n",
    "plot_time_series(merged_df, \"n34_diff1\", comparison = True)\n",
    "\n",
    "for month in range(1, 3):\n",
    "    print(f\"Standardized anomaly (calculated index) for forecast year: {forecast_year}, forecast month = {month} (based on data from month before):\")\n",
    "    print(standardize_index_diff1(sst_data, \"n34\", period_clm, year_fcst=forecast_year, month_init=month, before = True).sst.values)\n",
    "    print(f\"Reference value:\")\n",
    "    if month == 1:\n",
    "        print(n34_diff1_index_reference[(n34_diff1_index_reference[\"year\"] == forecast_year-1) & (n34_diff1_index_reference[\"month\"] == 12)].fl.values[0])\n",
    "    else:\n",
    "        print(n34_diff1_index_reference[(n34_diff1_index_reference[\"year\"] == forecast_year) & (n34_diff1_index_reference[\"month\"] == month-1)].fl.values[0])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368e447c",
   "metadata": {},
   "source": [
    "##### Index DMI_DIFF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0e2ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_dmi_diff1_df = prepare_time_series_data(sst_data, \"dmi\", period_clm, period_train, months, diff1 = True)\n",
    "\n",
    "# Reference values for index dmi_diff1\n",
    "dmi_diff1_index_reference = pd.read_csv(f\"{filepath_indices}dmi_diff1_full.csv\")\n",
    "time_series_dmi_diff1_reference_df = process_reference_index(dmi_diff1_index_reference, period_train)\n",
    "\n",
    "# Merge and plot dataframes\n",
    "merged_df = pd.merge(time_series_wpg_df, time_series_wpg_reference_df, on=['year', 'month'], suffixes=('_calculated', '_reference'))\n",
    "plot_time_series(merged_df, \"dmi_diff1\", comparison = True)\n",
    "\n",
    "for month in range(1, 3):\n",
    "    print(f\"Standardized anomaly (calculated index) for forecast year: {forecast_year}, forecast month = {month} (based on data from month before):\")\n",
    "    print(standardize_index_diff1(sst_data, \"dmi\", period_clm, year_fcst=forecast_year, month_init=month, before = True).sst.values)\n",
    "    print(f\"Reference value:\")\n",
    "    if month == 1:\n",
    "        print(dmi_diff1_index_reference[(dmi_diff1_index_reference[\"year\"] == forecast_year-1) & (dmi_diff1_index_reference[\"month\"] == 12)].fl.values[0])\n",
    "    else:\n",
    "        print(dmi_diff1_index_reference[(dmi_diff1_index_reference[\"year\"] == forecast_year) & (dmi_diff1_index_reference[\"month\"] == month-1)].fl.values[0])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f7d454",
   "metadata": {},
   "source": [
    "## ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680bddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precip data:\n",
    "# eofs_norm_anomalies_reshaped, numpy array of shape (eof = n_eofs, lat = 67, lon = 59) \n",
    "# pcs_ano_normal, numpy array of shape (year = train_end - train_start, n_eofs)\n",
    "# var_frac_ano_normal, numpy array of shape (n_eofs, )\n",
    "# Features/predictors: (dataframe with shape (year, month, standardized_anomaly))\n",
    "# time_series_{feature}_df for feature in ['n34','dmi','wvg','wsp','wpg','wp','wnp','n34_diff1','dmi_diff1','ueq850','ueq200','sji850','sji200']\n",
    "\n",
    "feature_names = ['n34','dmi','wvg','wsp','wpg','wp','wnp','n34_diff1','dmi_diff1','ueq850','ueq200','sji850','sji200']\n",
    "feature_dfs = {feature: eval(f\"time_series_{feature}_df\") for feature in feature_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910075a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplefilter(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc68b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation setup\n",
    "years_cv = list(range(year_train_start, year_train_end+1))\n",
    "years_verif = list(range(year_clm_start, year_clm_end+1))\n",
    "df_year = pd.DataFrame((np.array(years_cv) - 2000) / 10, index=years_cv, columns=['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bee072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of EOFs\n",
    "ntg = n_eofs\n",
    "\n",
    "# Calculate explained variance\n",
    "frac_expl_var = var_fracs / np.sum(var_fracs)\n",
    "wgt_values = np.sqrt(frac_expl_var)\n",
    "wgt = pd.DataFrame(np.tile(wgt_values, (len(years_verif), 1)), index=years_verif, columns=[f'eof{i+1}' for i in range(ntg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70753345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation folds\n",
    "\n",
    "k = 5\n",
    "nyr = len(years_cv)\n",
    "\n",
    "cv_folds = []\n",
    "for i in range(k):\n",
    "    idx_test = set(range(i*2, nyr, k*2)).union(set(range(1+i*2, nyr, k*2)))\n",
    "    idx_train = set(range(nyr)) - idx_test\n",
    "    cv_folds.append((list(idx_train), list(idx_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a148f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames for storing results\n",
    "df_fl_pred_mean = pd.DataFrame(index=years_verif, columns=[f'fl{i}' for i in range(1, ntg+1)])\n",
    "df_fl_pred_cov = pd.DataFrame(index=years_verif, columns=[f'cov-{i}{j}' for i in range(1, ntg+1) for j in range(1, ntg+1)])\n",
    "df_hyperparameters = pd.DataFrame(index=years_verif, columns=['alpha', 'l1_ratio'])\n",
    "df_selected_features = pd.DataFrame(index=years_verif, columns=['year'] + feature_names, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f696ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit models and make predictions\n",
    "previous_month = month_init - 1 if month_init > 1 else 12\n",
    "\n",
    "for iyr in years_verif:\n",
    "    df_target = pd.DataFrame(pcs[:, :ntg], index=years_cv).reindex(years_cv)\n",
    "    \n",
    "    df_combined_features = pd.DataFrame(index = years_cv)\n",
    "\n",
    "    # Select the data for the month before month_init and combine features\n",
    "    for feature in feature_names:\n",
    "        df_feature = feature_dfs[feature] \n",
    "        df_feature_selected = df_feature[df_feature['month'] == previous_month].set_index('year')['standardized_anomaly']\n",
    "        df_combined_features[feature] = df_feature_selected\n",
    "\n",
    "     # Add the standardized year as a predictor\n",
    "    # df_combined_features['year'] = (df_combined_features.index - 2000) / 10\n",
    "\n",
    "    # Ensure there are no missing values\n",
    "    df_combined_features.fillna(0, inplace=True)\n",
    "\n",
    "    y = df_target.to_numpy()\n",
    "    X = df_combined_features.to_numpy()\n",
    "\n",
    "    # Feature pre-selection\n",
    "    feature_idx = [True] + [False] * len(df_combined_features.columns)\n",
    "    for ift in range(len(feature_idx) - 1):\n",
    "        pval = [pearsonr(y[:, ipc], X[:, ift])[1] for ipc in range(ntg)]\n",
    "        feature_idx[1 + ift] = np.any(np.array(pval) < 0.1 * wgt.iloc[0, :])\n",
    "    \n",
    "    df_combined_features = df_combined_features.iloc[:, feature_idx[1:]]\n",
    "    df_year = pd.DataFrame((df_combined_features.index - 2000) / 10, index=df_combined_features.index, columns=['year'])\n",
    "    selected_columns = ['year'] + df_combined_features.columns.to_list()\n",
    "\n",
    "    # Ensure df_selected_features has the correct columns\n",
    "    if not set(selected_columns).issubset(df_selected_features.columns):\n",
    "        for col in selected_columns:\n",
    "            if col not in df_selected_features.columns:\n",
    "                df_selected_features[col] = 0\n",
    "\n",
    "    #print(f'{iyr}: {sum(feature_idx)} features selected')\n",
    "    if sum(feature_idx) == 1:\n",
    "        df_combined_features = df_year\n",
    "    else:\n",
    "        df_combined_features = pd.concat([df_year, df_combined_features], axis=1)\n",
    "\n",
    "    X = df_combined_features.to_numpy()\n",
    "    # Lasso regression\n",
    "    clf = MultiTaskLassoCV(cv=cv_folds, fit_intercept=False, max_iter=5000)\n",
    "    clf.fit(X, y)\n",
    "    df_hyperparameters.loc[iyr, 'alpha'] = clf.alpha_\n",
    "    df_hyperparameters.loc[iyr, 'l1_ratio'] = 1.0\n",
    "    ind_active = np.all(clf.coef_ != 0, axis=0)\n",
    "    #print(ind_active)\n",
    "    n_a = sum(ind_active)\n",
    "    #print(n_a)\n",
    "    if n_a > 0:\n",
    "        active_features = np.where(np.insert(ind_active, 0, True), 1, 0)  # Insert True for 'year' column\n",
    "        # Adjust the length of active_features to match selected_columns\n",
    "        if len(active_features) != len(selected_columns):\n",
    "            if len(active_features) < len(selected_columns):\n",
    "                active_features = np.append(active_features, [0] * (len(selected_columns) - len(active_features)))\n",
    "            else:\n",
    "                active_features = active_features[:len(selected_columns)]\n",
    "        \n",
    "        df_selected_features.loc[iyr, selected_columns] = active_features\n",
    "        dgf = 1 + n_a\n",
    "    else:\n",
    "        dgf = 1\n",
    "    df_coefficients = pd.DataFrame(clf.coef_, index=[f'eof{i}' for i in range(1,ntg+1)], columns=df_combined_features.columns)\n",
    "    #print(df_coefficients)\n",
    "    # Compute prediction covariance\n",
    "    errors = y - clf.predict(X)\n",
    "    df_fl_pred_cov.loc[iyr, :] = np.broadcast_to((np.dot(errors.T, errors) / (nyr - dgf)).flatten(), (1, ntg ** 2))\n",
    "\n",
    "#print(df_fl_pred_cov)\n",
    "# Save results\n",
    "# df_coefficients.to_csv(f'path_to_coefficients.csv')\n",
    "# df_fl_pred_cov.to_csv(f'path_to_fl_pred_cov.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65a1638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: CHIRPS vs. GPCC (short) vs. GPCC (long)\n",
    "\n",
    "month = 8\n",
    "season = 'OND'\n",
    "\n",
    "month_str = {1:\"January\", 2:\"February\", 3:\"March\", 4:\"April\", 5:\"May\", 6:\"June\", 7:\"July\", 8:\"August\", 9:\"September\", 10:\"October\", 11:\"November\", 12:\"December\"}[month]\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(14,4), width_ratios=[9, 8, 8])\n",
    "for i, target_prod, cv_period in zip([*range(3)],['chirps'],['1981-2020']):\n",
    "    df_coefficients = df_coefficients\n",
    "    nft = len(df_coefficients.columns)\n",
    "    ntg = len(df_coefficients.index)\n",
    "    img = ax[i].imshow(df_coefficients, vmin={'MAM':-80., 'JJAS':-160., 'OND':-230.}[season], vmax={'MAM':80., 'JJAS':160., 'OND':230.}[season], cmap='bwr', extent=[0,nft,0,ntg])\n",
    "    ax[i].set_xticks(np.arange(.5,nft))\n",
    "    ax[i].set_xticklabels(df_coefficients.columns.to_list(), rotation=90, fontsize=10)\n",
    "    ax[i].set_yticks(np.arange(ntg-.5,0,-1))\n",
    "    ax[i].set_yticklabels(df_coefficients.index.to_list(), fontsize=10)\n",
    "    ax[i].set_title(f'{target_prod.upper()}, training period: {cv_period}')\n",
    "\n",
    "fig.subplots_adjust(right=0.85)\n",
    "cbar_ax = fig.add_axes([0.9, 0.15, 0.03, 0.6])\n",
    "fig.colorbar(img, cax=cbar_ax)\n",
    "#fig.suptitle(f'LASSO regression coefficients for {season} forecast issued in {month_str}', fontsize=14)\n",
    "\n",
    "#plt.savefig(f'{data_dir}plots_ml_diagnostics/coef_training_period_comparison_im{month}_{season}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b53ddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tercile_probability_forecasts(season, year_fcst, month_init, period_clm, scaling, eofs, eigenvalues, coefficients, fl_eof_cov, ts_indices):\n",
    "\n",
    "    # Ensure columns and indices alignment\n",
    "    if not all(coefficients.columns == ts_indices.index):\n",
    "        raise ValueError(\"Columns of coefficients and indices of ts_indices are not aligned.\")\n",
    "    \n",
    "    # Calculate residual variance\n",
    "    var_eps = scaling**2 - np.sum(eigenvalues[:, None, None] * eofs**2, axis=0)\n",
    "\n",
    "    # Ensure ts_indices has year standardized\n",
    "    ts_indices['year'] = (year_fcst - 2000) / 10\n",
    "\n",
    "    # Calculate predictive mean of factor loadings\n",
    "    fl_eof_mean = coefficients.dot(ts_indices).to_numpy()\n",
    "\n",
    "    # Calculate mean and variance of the probabilistic forecast in normal space\n",
    "    mean_ml = np.sum(fl_eof_mean[:, None, None] * eofs, axis=0)\n",
    "    var_ml = np.sum(np.sum(fl_eof_cov[:, :, None, None] * eofs[None, :, :, :], axis=1) * eofs, axis=0) + var_eps\n",
    "\n",
    "    mean_ml = np.array(mean_ml, dtype=np.float64)\n",
    "    var_ml = np.array(var_ml, dtype=np.float64)\n",
    "\n",
    "    mean_ml_stdz = mean_ml / scaling\n",
    "    stdv_ml_stdz = np.sqrt(var_ml) / scaling\n",
    "\n",
    "    # Calculate tercile forecasts\n",
    "    prob_bn = norm.cdf((norm.ppf(0.333) - mean_ml_stdz) / stdv_ml_stdz)\n",
    "    prob_an = 1.0 - norm.cdf((norm.ppf(0.667) - mean_ml_stdz) / stdv_ml_stdz)\n",
    "\n",
    "    return prob_bn, prob_an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55f9ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simple(fields, titles, cmap, unit, year):\n",
    "    n_fields = len(fields)\n",
    "    fig, axes = plt.subplots(1, n_fields, figsize=(15, 5), subplot_kw={'projection': None})\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        im = ax.imshow(fields[i], extent=[lon.min(), lon.max(), lat.min(), lat.max()],\n",
    "                       origin='lower', cmap=cmap[i], vmin=0.333, vmax=1)\n",
    "        ax.set_title(titles[i])\n",
    "        ax.set_xlabel('Longitude')\n",
    "        ax.set_ylabel('Latitude')\n",
    "        cbar = fig.colorbar(im, ax=ax, orientation='vertical')\n",
    "        cbar.set_label(unit)\n",
    "\n",
    "    fig.suptitle(f'Predicted tercile probabilities for {season} precipitation amounts, {year}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd36294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percentiles(chirps_data, ref_period_indices):\n",
    "    # Extract the reference period indice\n",
    "    chirps_ref = chirps_data[ref_period_indices, :, :]\n",
    "    \n",
    "    # Calculate the 33rd and 67th percentiles for each grid point\n",
    "    percentile_33 = np.nanpercentile(chirps_ref, 33, axis=0)\n",
    "    percentile_67 = np.nanpercentile(chirps_ref, 67, axis=0)\n",
    "    \n",
    "    return percentile_33, percentile_67\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fea601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_precipitation(chirps_data, percentiles_33, percentiles_67, year):\n",
    "    # Select the specific year\n",
    "    actual_precip = chirps_data[year - year_train_start, :, :]\n",
    "    \n",
    "    # Categorize the precipitation\n",
    "    below_normal = actual_precip < percentiles_33\n",
    "    above_normal = actual_precip > percentiles_67\n",
    "    normal = (actual_precip >= percentiles_33) & (actual_precip <= percentiles_67)\n",
    "    \n",
    "    # Create a categorical array: 0 for below normal, 1 for normal, 2 for above normal\n",
    "    categories = xr.where(below_normal, 0, xr.where(above_normal, 2, 1))\n",
    "    \n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a2104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_predictions(prob_bn, prob_an, actual_categories):\n",
    "    # Determine the predicted category based on highest probability\n",
    "    pred_categories = xr.where(prob_bn > 0.4, 0, xr.where(prob_an > 0.4, 2, 1))\n",
    "    \n",
    "    # Compare predictions with actual categories\n",
    "    correct_predictions = pred_categories == actual_categories\n",
    "    \n",
    "    return correct_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520b420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_verification(verification, prec_data, lon, lat):\n",
    "    # Reintroduce NaNs based on the original prec_data\n",
    "    masked_verification = np.ma.masked_where(np.isnan(prec_data[year_fcst - year_train_start, :, :]), verification)\n",
    "    \n",
    "    # Create a custom colormap\n",
    "    cmap = mcolors.ListedColormap(['red', 'green', 'gray'])\n",
    "    bounds = [0, 0.5, 1.5, 2]\n",
    "    norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 5), subplot_kw={'projection': None})\n",
    "    \n",
    "    im = ax.imshow(masked_verification, extent=[lon.min(), lon.max(), lat.min(), lat.max()],\n",
    "                   origin='lower', cmap=cmap, norm=norm)\n",
    "    \n",
    "    ax.set_title(f'Verification of {season} {year_fcst} Precipitation Forecast')\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    \n",
    "    # Create a color bar with the correct labels\n",
    "    cbar = fig.colorbar(im, ax=ax, orientation='vertical', ticks=[0.25, 1, 1.75])\n",
    "    cbar.ax.set_yticklabels(['Incorrect', 'Correct', 'Masked'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7900c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "# Season, year_fcst, month_init, period_train, period_clm, ntg predefined\n",
    "# Model coeffiecients in df_coefficients\n",
    "# previous_month = month_init - 1 if month_init > 1 else 12\n",
    "year_fcst = 2020\n",
    "for year_fcst in range(year_clm_start, year_clm_end+1):\n",
    "    # Reshape the covariance matrix for each year\n",
    "    def reshape_covariance_matrix(cov_df, ntg):\n",
    "        reshaped_cov = np.zeros((len(cov_df), ntg, ntg))\n",
    "        for i, year in enumerate(cov_df.index):\n",
    "            reshaped_cov[i] = cov_df.loc[year].values.reshape(ntg, ntg)\n",
    "        return reshaped_cov\n",
    "\n",
    "    df_fl_pred_cov_reshaped = reshape_covariance_matrix(df_fl_pred_cov, ntg)\n",
    "\n",
    "    # Use the reshaped covariance matrix with the right year\n",
    "    cov_matrix_for_year = df_fl_pred_cov_reshaped[years_verif.index(year_fcst)]\n",
    "\n",
    "    scaling = np.nanstd(anomalies_normal[ref_period_indices, :, :], axis = 0, ddof = 1)\n",
    "\n",
    "    # Prepare ts_indices\n",
    "    ts_indices = pd.Series(index=df_coefficients.columns)\n",
    "    for feature in feature_names:\n",
    "        df_feature = feature_dfs[feature]\n",
    "        df_feature_selected = df_feature[df_feature['month'] == previous_month].set_index('year')['standardized_anomaly']\n",
    "        ts_indices[feature] = df_feature_selected.loc[year_fcst]\n",
    "\n",
    "    eigenvalues = var_frac_ano_normal  # The variance fraction for each EOF\n",
    "    # Ensure alignment before using the function\n",
    "    coefficients_columns = df_coefficients.columns.to_list()\n",
    "    ts_indices = ts_indices.reindex(coefficients_columns)\n",
    "\n",
    "    # Debugging statements\n",
    "    # print(\"Coefficient Columns:\", df_coefficients.columns)\n",
    "    # print(\"TS Indices Index:\", ts_indices.index)\n",
    "    # print(\"TS Indices:\", ts_indices)\n",
    "    # print(\"EOFs Shape:\", eofs_norm_anomalies_reshaped.shape)\n",
    "    # print(\"Eigenvalues Shape:\", eigenvalues.shape)\n",
    "    # print(\"Coefficients Shape:\", df_coefficients.shape)\n",
    "    # print(\"FL EOF Covariance Shape:\", df_fl_pred_cov_reshaped.shape)\n",
    "\n",
    "    prob_bn, prob_an = calculate_tercile_probability_forecasts(season, year_fcst, month_init, period_clm, scaling, eofs_norm_anomalies_reshaped, eigenvalues, df_coefficients, cov_matrix_for_year, ts_indices)\n",
    "\n",
    "    # Plot the probabilities\n",
    "    plot_simple(fields=[prob_bn, prob_an],\n",
    "                titles=['Below Normal', 'Above Normal'],\n",
    "                cmap=['Oranges', 'Greens'],\n",
    "                unit='Probability',\n",
    "                year=year_fcst)\n",
    "\n",
    "    # Define the reference period\n",
    "    reference_period = period_clm\n",
    "\n",
    "    # Calculate percentiles\n",
    "    percentiles_33, percentiles_67 = calculate_percentiles(prec_data, ref_period_indices)\n",
    "\n",
    "    actual_categories = categorize_precipitation(prec_data, percentiles_33, percentiles_67, year_fcst)\n",
    "    verification = verify_predictions(prob_bn, prob_an, actual_categories)\n",
    "\n",
    "    # Plot the verification\n",
    "    plot_verification(verification, prec_data, lon, lat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba52bb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from memory_profiler import profile\n",
    "\n",
    "def get_nearest_grid_index(lon_exmpl, lat_exmpl, lon_grid, lat_grid):\n",
    "    ix = np.argmin(abs(lon_grid-lon_exmpl))\n",
    "    iy = np.argmin(abs(lat_grid-lat_exmpl))\n",
    "    return ix, iy\n",
    "\n",
    "def get_xticks(x_extent, inc = 1):\n",
    "    x_inc = np.arange(-180,180,inc)\n",
    "    return(x_inc[np.where(np.logical_and(x_inc >= x_extent[0], x_inc <= x_extent[1]))])\n",
    "\n",
    "def get_yticks(y_extent, inc = 1):\n",
    "    y_inc = np.arange(-90,90,inc)\n",
    "    return(y_inc[np.where(np.logical_and(y_inc >= y_extent[0], y_inc <= y_extent[1]))])\n",
    "\n",
    "@profile\n",
    "def plot_fields(fields_list, lon, lat, lon_bounds, lat_bounds, main_title, subtitle_list, unit, vmin=None, vmax=None, cmap='BuPu', water_bodies=False, ticks=True, tick_labels=None):\n",
    "\n",
    "    n_img = len(fields_list)\n",
    "    img_extent = lon_bounds + lat_bounds\n",
    "\n",
    "    if not type(unit) is list:\n",
    "        unit = [unit for i in range(n_img)]\n",
    "\n",
    "    if not type(cmap) is list:\n",
    "        cmap = [cmap for i in range(n_img)]\n",
    "\n",
    "    if vmin == None:\n",
    "        vmin = [np.nanmin(field) for field in fields_list]\n",
    "    if vmax == None:\n",
    "        vmax = [np.nanmax(field) for field in fields_list]\n",
    "\n",
    "    if not type(vmin) is list:\n",
    "        vmin = [vmin for i in range(n_img)]\n",
    "    if not type(vmax) is list:\n",
    "        vmax = [vmax for i in range(n_img)]       \n",
    "\n",
    "    if ticks == True:\n",
    "        ticks = [ticks for i in range(n_img)]\n",
    "    elif not type(ticks) is list:\n",
    "        print(\"Error! Argument 'ticks' must be a list or a list of lists.\")\n",
    "    elif all([isinstance(tt, float) or isinstance(tt, int) for tt in ticks]):\n",
    "        ticks = [ticks for i in range(n_img)]\n",
    "\n",
    "    if tick_labels == None:\n",
    "        tick_labels = [tick_labels for i in range(n_img)]\n",
    "    elif not type(tick_labels) is list:\n",
    "        print(\"Error! Argument 'ticks_labels' must be a list or a list of lists.\")\n",
    "    elif all([isinstance(tt, float) or isinstance(tt, int) or isinstance(tt, str) for tt in tick_labels]):\n",
    "        tick_labels = [tick_labels for i in range(n_img)]\n",
    "\n",
    "    r = abs(lon[1]-lon[0])\n",
    "    lons_mat, lats_mat = np.meshgrid(lon, lat)\n",
    "    lons_matplot = np.hstack((lons_mat - r/2, lons_mat[:,[-1]] + r/2))\n",
    "    lons_matplot = np.vstack((lons_matplot, lons_matplot[[-1],:]))\n",
    "    lats_matplot = np.hstack((lats_mat, lats_mat[:,[-1]]))\n",
    "    lats_matplot = np.vstack((lats_matplot - r/2, lats_matplot[[-1],:] + r/2))     # assumes latitudes in ascending order\n",
    "\n",
    "    dlon = (lon_bounds[1]-lon_bounds[0]) // 8\n",
    "    dlat = (lat_bounds[1]-lat_bounds[0]) // 8\n",
    "\n",
    "    fig_height = 7.\n",
    "    fig_width = (n_img*1.15)*(fig_height/1.1)*np.diff(lon_bounds)[0]/np.diff(lat_bounds)[0]\n",
    "\n",
    "    fig = plt.figure(figsize=(fig_width,fig_height))\n",
    "    for i_img in range(n_img):\n",
    "        ax = fig.add_subplot(100+n_img*10+i_img+1, projection=ccrs.PlateCarree())\n",
    "        cmesh = ax.pcolormesh(lons_matplot, lats_matplot, fields_list[i_img], cmap=cmap[i_img], vmin=vmin[i_img], vmax=vmax[i_img])\n",
    "        ax.set_extent(img_extent, crs=ccrs.PlateCarree())\n",
    "        ax.set_yticks(get_yticks(img_extent[2:4],dlat), crs=ccrs.PlateCarree())\n",
    "        ax.yaxis.set_major_formatter(LatitudeFormatter()) \n",
    "        ax.set_xticks(get_xticks(img_extent[0:2],dlon), crs=ccrs.PlateCarree())\n",
    "        ax.xaxis.set_major_formatter(LongitudeFormatter(zero_direction_label=True))\n",
    "        ax.add_feature(cfeature.COASTLINE, linewidth=2)\n",
    "        ax.add_feature(cfeature.BORDERS, linewidth=2, linestyle='-', alpha=.9)\n",
    "        if water_bodies:\n",
    "            ax.add_feature(cfeature.LAKES, alpha=0.95)\n",
    "            ax.add_feature(cfeature.RIVERS)\n",
    "\n",
    "        plt.title(subtitle_list[i_img], fontsize=14)\n",
    "        divider = make_axes_locatable(ax)\n",
    "        ax_cb = divider.new_horizontal(size=\"5%\", pad=0.1, axes_class=plt.Axes)\n",
    "        fig.add_axes(ax_cb)\n",
    "        cbar = plt.colorbar(cmesh, cax=ax_cb)\n",
    "        cbar.set_label(unit[i_img])\n",
    "        if not ticks[i_img] == True:\n",
    "            cbar.set_ticks(ticks[i_img])\n",
    "            cbar.set_ticklabels(tick_labels[i_img])\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    plt.tight_layout(rect=[0,0,1,0.95])\n",
    "    fig.suptitle(main_title, fontsize=16)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cbcc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_fields(fields_list = [prob_bn, prob_an],\n",
    "#           lon = lon,\n",
    "#           lat = lat,\n",
    "#           lon_bounds = lon_bnds,\n",
    "#           lat_bounds = lat_bnds,\n",
    "#           main_title = f'Predicted tercile probabilities for {season} precipitation amounts',\n",
    "#           subtitle_list = ['below normal','above normal'],\n",
    "#           vmin = 0.333,\n",
    "#           vmax = 1,\n",
    "#           cmap = ['Oranges','Greens'],\n",
    "#           unit = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccd1eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#data_dir = '/home/michael/nr/samba/PostClimDataNoBackup/CONFER/EASP/'\n",
    "\n",
    "# month = month_init                # month in which the forecast is issued; predictors are taken from previous month\n",
    "# season_name = season\n",
    "# ref_period = f'{year_clm_start}-{year_clm_end}'\n",
    "\n",
    "\n",
    "# print(f'\\n Generating {season} forecasts at halfdeg resolution')\n",
    "\n",
    "# # -- Calculate explained variance by the respective EOFs-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# refper_start = int(ref_period[:4])\n",
    "# refper_end = int(ref_period[-4:])\n",
    "\n",
    "# nc = xr.open_dataset(f'{data_dir}eofs/chirps/halfdeg_res/refper_{ref_period}/prec_loyo_seasonal_{season}.nc')\n",
    "# nc_subset = nc.sel(loy=slice(years_verif[0],years_verif[-1]))\n",
    "# eigenvalues = (nc_subset.d.values**2) / (refper_end-refper_start)\n",
    "# nc.close()\n",
    "\n",
    "# for iyr in years_verif:\n",
    "# #    print(iyr)\n",
    "#     df_target = data_prec.loc[(slice(None),iyr,range(1,ntg+1)),:].unstack().droplevel(level=1).droplevel(level=0, axis=1).fillna(0.).reindex(years_cv)\n",
    "#     data_indices_yr = []\n",
    "#     for i in range(len(data_indices)):\n",
    "#         data_indices_yr.append(data_indices[i].loc[(slice(None),month-1,iyr),:].droplevel(level=(1,2)))\n",
    "#     df_features = pd.concat(data_indices_yr, axis=1).fillna(0.).reindex(years_cv)\n",
    "#     y = df_target.to_numpy()\n",
    "#     X = df_features.to_numpy()\n",
    "#    # Feature pre-selection\n",
    "#     feature_idx = [True]+[False]*len(df_features.columns)\n",
    "#     for ift in range(len(feature_idx)-1):\n",
    "#         pval = [pearsonr(y[:,ipc], X[:,ift])[1] for ipc in range(ntg)]\n",
    "#         feature_idx[1+ift] = np.any(np.array(pval)<0.1*wgt.loc[iyr])\n",
    "#     df_features = df_features.iloc[:,feature_idx[1:]]\n",
    "#     df_selected_features.loc[iyr,['year']+df_features.columns.to_list()] = 0\n",
    "#     print(f'{iyr}: {sum(feature_idx)} features selected')\n",
    "#    # Interactions with b/n/a dummy variables of features\n",
    "#     if sum(feature_idx) == 1:\n",
    "#         df_features = df_year\n",
    "#     else:\n",
    "#         df_features = pd.concat([df_year, df_features], axis=1)\n",
    "\n",
    "#     X = df_features.to_numpy()\n",
    "\n",
    "#     # Lasso regression of pre-selected features\n",
    "#     clf = MultiTaskLassoCV(cv=cv_folds, fit_intercept=False, max_iter=5000)\n",
    "#     clf.fit(X, y)\n",
    "#     df_hyperparameters.loc[iyr,'alpha'] = clf.alpha_\n",
    "#     df_hyperparameters.loc[iyr,'l1_ratio'] = 1.\n",
    "#     ind_active = np.all(clf.coef_!=0, axis=0)\n",
    "#     n_a = sum(ind_active)\n",
    "#     if n_a > 0:\n",
    "#         df_selected_features.loc[iyr,df_features.columns.to_list()] = np.where(ind_active, 1, 0)\n",
    "#         dgf = 1 + n_a\n",
    "#     else:\n",
    "#         dgf = 1\n",
    "#    # In 'full' mode, we mainly care about the estimated coefficients\n",
    "#     df_coefficients = pd.DataFrame(clf.coef_, index=[f'eof{i}' for i in range(1,ntg+1)], columns=df_features.columns)\n",
    "#     errors = y - clf.predict(X)\n",
    "#     df_fl_pred_cov.loc[:,:] = np.broadcast_to((np.dot(errors.T,errors)/(nyr-dgf)).flatten(), (len(years_verif),ntg**2))\n",
    "#     break\n",
    "\n",
    "\n",
    "# # if not path.exists(f'{data_dir}fls_pred/chirps/seasonal/halfdeg_res'):\n",
    "# #     os.mkdir(f'{data_dir}fls_pred/chirps/seasonal/halfdeg_res')\n",
    "\n",
    "# # if not path.exists(f'{data_dir}fls_pred/chirps/seasonal/halfdeg_res/refper_{ref_period}_cvper_{years_cv[0]}-{years_cv[-1]}'):\n",
    "#     os.mkdir(f'{data_dir}fls_pred/chirps/seasonal/halfdeg_res/refper_{ref_period}_cvper_{years_cv[0]}-{years_cv[-1]}')\n",
    "\n",
    "# df_coefficients.to_csv(f'{data_dir}fls_pred/chirps/seasonal/halfdeg_res/refper_{ref_period}_cvper_{years_cv[0]}-{years_cv[-1]}/coefficients_{predictor}_lasso_full_im{month}_{season}.csv')\n",
    "# df_fl_pred_cov.to_csv(f'{data_dir}fls_pred/chirps/seasonal/halfdeg_res/refper_{ref_period}_cvper_{years_cv[0]}-{years_cv[-1]}/fls_cov_{predictor}_lasso_full_im{month}_{season}.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47ca337",
   "metadata": {},
   "source": [
    "Run LASSO model to predict precipitation EOFs based on indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37aca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ec6f69",
   "metadata": {},
   "source": [
    "Visualize fitted coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950e58c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e7471e",
   "metadata": {},
   "source": [
    "Load indices for the forecast year and use the previously fitted model to make a forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9853a8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prob_fcst_below, prob_fcst_above = calculate_tercile_probability_forecasts(season, year_fcst, month_init, period_train, period_clm, indices_dir, anomaly_dir, eof_dir, fcst_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e326602",
   "metadata": {},
   "source": [
    "Depict as a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cf4aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_fields (fields_list = [prob_fcst_below, prob_fcst_above],\n",
    "#           lon = lon,\n",
    "#           lat = lat,\n",
    "#           lon_bounds = lon_bnds,\n",
    "#           lat_bounds = lat_bnds,\n",
    "#           main_title = f'Predicted tercile probabilities for {season} precipitation amounts',\n",
    "#           subtitle_list = ['below normal','above normal'],\n",
    "#           vmin = 0.333,\n",
    "#           vmax = 1,\n",
    "#           cmap = ['Oranges','Greens'],\n",
    "#           unit = '')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (confer-wp3-env)",
   "language": "python",
   "name": "confer-wp3-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
